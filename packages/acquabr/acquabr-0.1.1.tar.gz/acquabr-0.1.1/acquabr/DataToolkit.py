import os
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore", category=RuntimeWarning)

class DataToolkit:

    def delFiles(folder_path= None, del_folder= False):
        """
        This method allows for the deletion of data generated by the 'HidroWeb.getData()' method,
        either in a user-specified directory or the default directory 'acquabr'.
        It facilitates the clearing of the folder from files obtained during download and extraction processes.
        This procedure aims to ensure that the directory is empty before starting the update or processing of new files,
        preventing conflicts or errors during the process.

        Parameters:
        folder_path (str, optional): The path of the folder to clear data from. The default is 'acquabr'.
        del_folder (bool, optional): If True, deletes the entire folder along with its contents.

        Notes:
        If no folder_path is provided, the method will operate on the default directory 'acquabr'.
        If the specified folder does not exist, the method will display a message accordingly.
        If any errors occur during the file deletion process, an error message will be displayed.

        Example:
        To clear data in the default 'acquabr' folder:
        DataToolkit.delFiles()

        To clear data in a specific folder:
        DataToolkit.delFiles('CustomFolder')

        To delete the entire folder and its contents:
        DataToolkit.delFiles('CustomFolder', del_folder=True)
        """
        # Block 0: Cleaning and Removal of Old Files
        if folder_path== None:
          folder_path = "acquabr"
        if os.path.exists(folder_path):
            try:
                if os.path.isdir(folder_path):
                    if del_folder:
                        os.rmdir(folder_path)
                        print(f"Folder {folder_path} and its contents successfully deleted.")
                    else:
                        files = os.listdir(folder_path)
                        for file in files:
                            file_path = os.path.join(folder_path, file)
                            if os.path.isfile(file_path):
                                os.remove(file_path)
                                print(f"File {file} successfully deleted.")
                        print("The all files have been deleted.")
                else:
                    print("The path is not a valid directory.")
            except Exception as e:
                print(f"An error occurred while deleting the files: {str(e)}")
        else:
            if folder_path != "acquabr":
                print(f"Folder: {folder_path} does not exist.")

    def resampleSerie(path_data,
                      time=None,
                      method=None,
                      start_date=None,
                      final_date=None,
                      min_count=None,
                      threshold= None ,
                      fill= None):
        """
        Resamples a time series DataFrame using specified parameters.

        This function performs resampling on a given time series DataFrame, applying various statistics methods based on the specified parameters.

        Parameters:
        path_data (pandas.DataFrame): The input DataFrame containing the time series data.
        time (str or None, optional): The resampling frequency (e.g., 'D' for daily, 'M' for monthly, 'Y' for yearly).
        method (str or None, optional): The statistical method to be applied during resampling (e.g., 'MEAN', 'SUM', 'MEDIAN').
        start_date (str or None,optional ): The start date for the resampling period.
        final_date (str or None, optional): The final date for the resampling period.
        min_count (int or None, optional ): The minimum number of valid data points required for resampling method 'SUM' that use it (default 1)
        threshold (float or None, optional): The threshold value for selecting data points in resampling method 'MIN'that use it. (default 0)
        fill (float or None, optional): The value to fill NaN values in the resampled DataFrame.

        Returns:
        pandas.DataFrame: The resampled DataFrame based on the specified parameters.

        Notes:
        - The 'time' parameter defines the resampling frequency using time units.
        - The 'method' parameter defines the statistical method to be applied during resampling.
        - The 'start_date' and 'final_date' parameters define the range of dates for resampling.
        - The 'fill', 'min_count', and 'threshold' parameters are used for data processing during resampling.

        Example:
        resampled_data = DataToolkit.resampleSerie(input_data, time='M', method='MEAN', start_date='2000-01-01', final_date='2005-12-31', fill=0, min_count=5)
        """
        # Block 0: Function Parametes Validations
        if type(path_data) != pd.DataFrame:
            raise TypeError("The 'path_data' variable must be a pandas DataFrame.")

        if not pd.api.types.is_datetime64_any_dtype(path_data.index):
            raise ValueError("The DataFrame 'path_data' index must contain datetime values.")

        # Block 1: Function Input Parametes
        df= path_data.copy()

        if threshold is None:
            threshold= 0

        if min_count is None:
            min_count= 1

        # Block 2: Statistics Methods Mapping for Data Aggregation and Calculation
        method_mapping = {
                        "COUNT": 'count',
                        "COUNT_NAN": lambda x: x.isna().sum(),
                        "RANGE": lambda x: x.max() - x[x >= threshold].min(),
                        "SUM": lambda x: x.sum(min_count=min_count),
                        "MAX": lambda x: np.nanmax(x),
                        "MEAN": 'mean',
                        "MEDIAN": 'median',
                        "HAR_MEAN": lambda x: len(x) / np.sum(1 / x),
                        "GEO_MEAN": lambda x: np.exp(np.mean(np.log(x))),
                        "MIN": lambda x: x[x >= threshold].min(),
                        "STD": 'std',
                        "VAR": 'var',
                        "CV": lambda x: x.std() / x.mean(),
                        "MAE": lambda x: abs(x - x.mean()).mean(),
                        "MSE": lambda x: ((x - x.mean())**2).mean(),
                        "SKEW": 'skew',
                        "KURTOSIS": 'kurtosis',
                        "QUAN_25": lambda x: np.nanquantile(x, 0.25),
                        "QUAN_50": lambda x: np.nanquantile(x, 0.50),
                        "QUAN_75": lambda x: np.nanquantile(x, 0.75),
                        "IQR": lambda x: np.nanquantile(x, 0.75) - np.nanquantile(x, 0.25),
                        "QUAN_SUP_T": lambda x: np.nanquantile(x, 0.75) + 1.5 * (np.nanquantile(x, 0.75) - np.nanquantile(x, 0.25)),
                        "QUAN_INF_T": lambda x: np.nanquantile(x, 0.25) - 1.5 * (np.nanquantile(x, 0.75) - np.nanquantile(x, 0.25)),
                    }

        # Block 3: Data Filtering
        if start_date is not None:
            try:
                df= df.loc[start_date:final_date]
            except KeyError as e:
                raise ValueError(f"Invalid start_date or final_date argument: {e}")

        # Block 4: Resampling and Aggregation of Temporal Data
        if time is not None and method is not None:
            if method not in method_mapping:
                available_methods = ", ".join(sorted(method_mapping.keys()))
                raise KeyError(f"Invalid statistical method '{method}'. Available options are: {available_methods}")
            else:
                aggregation_function= method_mapping[method]
                df= df.resample(rule=time).agg(aggregation_function)

        # Block 5: Filling Missing Values
        if fill is not None:
            df.fillna(value=fill, inplace=True)

        return df

    def rollingSerie(path_data,
                 window= None,
                 method= None,
                 min_periods=None,
                 center=False,
                 on=None,
                 axis=0,
                 start_date=None,
                 final_date=None,
                 min_count=None,
                 threshold=None,
                 fill=None):
        """
        Performs rolling window calculations on a given time series DataFrame using specified parameters.
        This function performs rolling window calculations on a given time series DataFrame, applying various methods
        based on the specified parameters.

        Parameters:
        path_data (pandas.DataFrame): The input DataFrame containing the time series data.
        window (int, optional): The size of the rolling window.
        method (str or None, optional): The statistical method to be applied during rolling (e.g., 'MEAN', 'SUM', 'MEDIAN').
        min_periods (int or None, optional): The minimum number of observations required for a valid result.
        center (bool, default False): Determines if the window labels are set at the center of the window index.
        on (str, optional): Specifies the column label or Index level on which to calculate the rolling window.
        axis (int or str, default 0): Determines if the rolling operation is performed across rows (0 or 'index') or columns (1 or 'columns').
        start_date (str or None, optional): The starting date for the rolling window calculations.
        final_date (str or None, optional): The final date for the rolling window calculations.
        min_count (int or None, optional): The minimum number of valid data points required for resampling method 'SUM' that use it (default 1)
        threshold (float or None, optional): The threshold value for selecting data points in resampling method 'MIN'that use it. (default 0)
        fill (float or None, optional): The value used to fill NaN values in the resulting DataFrame.

        Returns:
        pandas.DataFrame: The DataFrame with rolling window calculations based on the specified parameters.

            Notes:
        - The 'window' parameter defines the size of the rolling window.
        - The 'method' parameter defines the statistical method to be applied during rolling.
        - The 'min_periods', 'center', 'on', and 'axis' parameters influence the rolling behavior.
        - The 'start_date' and 'final_date' parameters define the range of dates for the rolling calculations.
        - The 'min_count' parameter specifies the minimum number of valid data points for certain resampling methods.
        - The 'threshold' parameter is used to select data points based on a specified threshold value.
        - The 'fill' parameter fills NaN values in the resulting DataFrame.

        Example:
        rolled_data = DataToolkit.rollingSerie(input_data, window=30, method='MEAN', start_date='2000-01-01', final_date='2020-12-31', fill=0)
        """

        # Block 0: Function Parametes Validations
        if type(path_data) != pd.DataFrame:
            raise TypeError("The 'path_data' variable must be a pandas DataFrame.")

        if not pd.api.types.is_datetime64_any_dtype(path_data.index):
            raise ValueError("The DataFrame 'path_data' index must contain datetime values.")

        # Block 1: Function Input Parametes
        df= path_data.copy()

        if threshold is None:
            threshold = 0

        if min_count is None:
            min_count = 1

        # Block 2: Statistics Methods Mapping for Rolling Window Calculation
        method_mapping = {
                        "COUNT": 'count',
                        "COUNT_NAN": lambda x: x.isna().sum(),
                        "RANGE": lambda x: x.max() - x[x >= threshold].min(),
                        "SUM": lambda x: x.sum(min_count=min_count),
                        "MAX": lambda x: np.nanmax(x),
                        "MEAN": 'mean',
                        "MEDIAN": 'median',
                        "HAR_MEAN": lambda x: len(x) / np.sum(1 / x),
                        "GEO_MEAN": lambda x: np.exp(np.mean(np.log(x))),
                        "MIN": lambda x: x[x >= threshold].min(),
                        "STD": 'std',
                        "VAR": 'var',
                        "CV": lambda x: x.std() / x.mean(),
                        "MAE": lambda x: abs(x - x.mean()).mean(),
                        "MSE": lambda x: ((x - x.mean())**2).mean(),
                        "SKEW": 'skew',
                        "KURTOSIS": 'kurtosis',
                        "QUAN_25": lambda x: np.nanquantile(x, 0.25),
                        "QUAN_50": lambda x: np.nanquantile(x, 0.50),
                        "QUAN_75": lambda x: np.nanquantile(x, 0.75),
                        "IQR": lambda x: np.nanquantile(x, 0.75) - np.nanquantile(x, 0.25),
                        "QUAN_SUP_T": lambda x: np.nanquantile(x, 0.75) + 1.5 * (np.nanquantile(x, 0.75) - np.nanquantile(x, 0.25)),
                        "QUAN_INF_T": lambda x: np.nanquantile(x, 0.25) - 1.5 * (np.nanquantile(x, 0.75) - np.nanquantile(x, 0.25)),
                    }

        # Block 2: Date Range Filtering
        if start_date is not None:
            try:
                df= df.loc[start_date:final_date]
            except Exception as e:
                raise ValueError(f"Invalid start_date or final_date argument: {str(e)}")

        # Block 3: Rolling Window Calculation
        if window is not None and method is not None:
            if method not in method_mapping:
                available_methods = ", ".join(sorted(method_mapping.keys()))
                raise KeyError(f"Invalid statistical method '{method}'. Available options are: {available_methods}")
            else:
                aggregation_function = method_mapping[method]
                df= df.rolling(window=window,min_periods=min_periods,center=center,on=on,axis=axis).agg(aggregation_function)

        # Block 4: Filling Missing Values
        if fill is not None:
            df.fillna(value=fill, inplace=True)

        return df

    def describeStatistic(path_data,
                           threshold= None,
                           individual=False):

        """
        Generate descriptive statistics for a given dataset.

        Parameters:
        path_data (pandas.DataFrame): The input dataset containing the data to be analyzed.
        threshold (float, optional): The threshold value for selecting data points in resampling method 'MIN'that use it. (default 0)
        individual (bool, optional): Whether to analyze data individually for each station (True) or collectively (False). Default is True.

        Returns:
        pandas.DataFrame: A DataFrame containing descriptive statistics for the given dataset.

        Notes:
        - This function computes various statistics including range, sum, max, mean, median, min, standard deviation, coefficient of variation,
          skewness, kurtosis, and quantiles, based on the specified method_mapping dictionary.
        - The 'type' parameter should describe the type of data being analyzed.
        - If 'individual' is set to False, the calculations are performed collectively for the entire dataset.
        - The 'method_mapping' dictionary defines the statistics to be calculated.

        Example:
        rolled_data = DataToolkit.describeStatistic(input_data, min_count=365, threshold= 1, individual=True)
        """
        # Block 0: Function Parametes Validations
        if type(path_data) != pd.DataFrame:
            raise TypeError("The 'path_data' variable must be a pandas DataFrame.")

        if not pd.api.types.is_datetime64_any_dtype(path_data.index):
            raise ValueError("The DataFrame 'path_data' index must contain datetime values.")

        if (path_data < 0).any().any():
            raise ValueError("The DataFrame 'path_data' should not contain negative values.")

        # Block 1: Function Input Parametes
        df= path_data.copy()

        if threshold is None:
            threshold= 0

        # Block 2: Statistics Methods Mapping
        method_mapping = {
            "RANGE": lambda x: x.max() - x[x >=threshold].min(),
            "SUM": lambda x: x.sum(),
            "MAX": lambda x: np.nanmax(x),
            "MEAN": 'mean',
            "MEDIAN": 'median',
            "HAR_MEAN": lambda x: len(x) / np.sum(1 / x),
            "GEO_MEAN": lambda x: np.exp(np.mean(np.log(x))),
            "MIN": lambda x: x[x >= threshold].min(),
            "STD": 'std',
            "VAR": 'var',
            "CV": lambda x: x.std() / x.mean(),
            "MAE": lambda x: abs(x - x.mean()).mean(),
            "MSE": lambda x: ((x - x.mean())**2).mean(),
            "SKEW": 'skew',
            "KURTOSIS": 'kurtosis',
            "QUAN_25": lambda x: np.nanquantile(x, 0.25),
            "QUAN_50": lambda x: np.nanquantile(x, 0.50),
            "QUAN_75": lambda x: np.nanquantile(x, 0.75),
            "IQR": lambda x: np.nanquantile(x, 0.75) - np.nanquantile(x, 0.25),
            "QUAN_SUP_T": lambda x: np.nanquantile(x, 0.75) + 1.5 * (np.nanquantile(x, 0.75) - np.nanquantile(x, 0.25)),
            "QUAN_INF_T": lambda x: np.nanquantile(x, 0.25) - 1.5 * (np.nanquantile(x, 0.75) - np.nanquantile(x, 0.25)),

        }

        # Block 3: Generating Summary Statistics for Time Series Data
        processed_data = []
        for column in df.columns:
            station_code = column
            first_record = df[column].first_valid_index()
            last_record = df[column].last_valid_index()

            if individual== False:
                first_record = df[column].index[0]
                last_record = df[column].index[-1]

            first_record_year = first_record.year if first_record else None
            last_record_year = last_record.year if last_record else None

            if first_record_year is not None and last_record_year is not None:
                interval_year= int(last_record.year)-int(first_record.year) + 1
            else:
                interval_year = None

            missing_count = (df[column].loc[first_record:last_record].isnull() | (df[column].loc[first_record:last_record] < 0)).sum()
            data_count = df[column].loc[first_record:last_record].count() + missing_count

            if data_count == 0:
                data_count= np.nan
                missing_percentage = np.nan
                missing_count= np.nan
            else:
                missing_percentage = (missing_count / data_count) * 100

            summary_stats_agg = [func(df[column].loc[first_record:last_record]) if callable(func) else df[column].loc[first_record:last_record].agg(func) for func in  method_mapping.values()]
            processed_data.append((station_code, first_record, last_record, first_record_year, last_record_year,interval_year ,data_count, missing_count, missing_percentage, *summary_stats_agg))

        # Block 3: Creating Summary DataFrame with Station Data and Calculated Statistics
        columns = ['STATIONS', 'FIRST_DATE', 'LAST_DATE', 'FIRST_YEAR', 'LAST_YEAR', 'YEARS' ,'DATA_COUNT', 'MISSING', 'MISS_PERC'] + list(method_mapping.keys())
        summary= pd.DataFrame(processed_data, columns=columns)
        return summary


    def describeAssociation(path_data,
                            measure,
                            method= None):
        """
        Calculates the correlation or covariance matrix for a dataset.

        Parameters:
        path_data (pandas.DataFrame): The DataFrame containing the data for analysis.
        measure (str): The measure to be calculated. Can be 'CORR' for correlation or 'COV' for covariance.
        method (str): The method to be used for calculation (e.g., 'pearson', 'spearman', 'kendall' for correlation).

        Returns:
        pandas.DataFrame: The correlation or covariance matrix between the variables in the DataFrame.

        Example:
        data_assoc = DataToolkit.describeAssociation(input_data, measure='CORR',  method= 'kendall')
        """
        # Block 0: Function Parametes Validations
        if type(path_data) != pd.DataFrame:
            raise TypeError("The 'path_data' variable must be a pandas DataFrame.")

        if not pd.api.types.is_datetime64_any_dtype(path_data.index):
            raise ValueError("The DataFrame 'path_data' index must contain datetime values.")

        if method is not None:
            if method not in ["PEARSON", "SPEARMAN", "KENDALL"]:
                raise ValueError("The method must be 'pearson', 'spearman', or 'kendall'.")

        # Block 1: Function Input Parametes
        df= path_data.copy()

        if method == "PEARSON":
            method='pearson'
        elif method == "SPEARMAN":
            method='spearman'
        else:
            method='kendall'

        # Block 2: Calculation of Correlation or Covariance Matrix
        if measure == "CORR":
            matrix_function = pd.DataFrame.corr
            matrix = matrix_function(df, method)
        elif measure == "COV":
            if method is None:
                matrix_function = pd.DataFrame.cov
                matrix = matrix_function(df)
            else:
                raise ValueError("'COV' does not have methodologies in this case.")
        else:
            raise ValueError("The measure must be 'CORR' or 'COV'.")

        return matrix

    def searchWindow(path_data,
                      window= None,
                      min_count= None,
                      remove_empty_col=False
                             ):

        """
        Performs a windowed search for the best data window in a given time series DataFrame based on missing data.

        This function performs a windowed search for the optimal data window within a time series DataFrame,
        aiming to find a time interval with the least amount of missing data.
        It aggregates the data on an annual basis (or as specified),
        calculates missing data counts for different window positions,
        and selects the best-performing window for further analysis.

        Parameters:
        path_data (pandas.DataFrame): The input DataFrame containing the time series data.
        window (int, optional): The size of the rolling window for the search.
        min_count (int or None, optional): The minimum number of valid observations required for a valid result.
        remove_empty_columns (bool, default False): Whether to remove empty columns (all NaN values) from the filtered DataFrame.

        Returns:
        pandas.DataFrame: The filtered DataFrame containing data within the best-performing window.

        Notes:
        - The input DataFrame index must contain datetime values.
        - The input data frequency must be daily.

        Example:
        filtered_data = DataToolkit.searchWindow(input_data, window=15, min_count=5, remove_empty_columns=True)
        """

        # Block 0: Function Parametes Validations
        if type(path_data) != pd.DataFrame:
            raise TypeError("The 'path_data' variable must be a pandas DataFrame.")

        if not pd.api.types.is_datetime64_any_dtype(path_data.index):
            raise ValueError("The DataFrame 'path_data' index must contain datetime values.")

        if path_data.index.freq != 'D':
            raise ValueError("Input data (path_data) must be in daily frequency (step).")

        if (path_data < 0).any().any():
            raise ValueError("The DataFrame 'path_data' should not contain negative values.")

        # Block 1: Function Input Parametes
        df= path_data.copy()
        df_original= path_data.copy()

       # Block 2: Data Aggregation and Windowing for Time Series Analysis
        if min_count is None:
            df.fillna(value=-9999, inplace=True)
            df = df.resample(rule="Y").sum()
            df[df < 0] = np.nan
        else:
            df = df.resample(rule="Y").sum(min_count=min_count)

        if window is None:
            window= 10

        # Block 3: Windowed Search for Missing Data
        def search(df, window_size):
            result = []
            index = df.index

            for i in range(len(index) - window_size + 1):
                window = index[i:i + window_size]
                window_data = df.loc[window]

                missing_data_count = window_data.isnull().sum().sum()
                result.append((window, missing_data_count))

            sorted_result = sorted(result, key=lambda x: x[1])
            return sorted_result

        # Block 4: Visualization of Window Search Results
        result = search(df, window_size= window)
        for window, missing_count in result:
            print(f"Window: {window.min()} - {window.max()}, Missing Data Count: {missing_count}")

        # Block 5: Selecting the Best Window and Filtering the Original DataFrame
        best_window, _ = result[0]
        start_year = best_window[0].year
        final_year = best_window[-1].year
        start_date = pd.Timestamp(year=start_year, month=1, day=1)
        end_date = pd.Timestamp(year=final_year, month=12, day=31)
        best_window_original = pd.date_range(start=start_date, end=end_date)
        filtered_df = df_original.loc[best_window_original]

        # Block 6: Removing Empty Columns (Optional)
        if remove_empty_col:
            filtered_df = filtered_df.dropna(axis=1, how='all')

        return filtered_df

