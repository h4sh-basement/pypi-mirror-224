Metadata-Version: 2.1
Name: torchserve-client
Version: 0.0.2
Summary: Python Client for TorchServe APIs
Home-page: https://github.com/Ankur-singh/torchserve_client
Author: Ankur Singh
Author-email: as.ankursingh3.1@gmail.com
License: Apache Software License 2.0
Keywords: nbdev jupyter notebook python
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Natural Language :: English
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: License :: OSI Approved :: Apache Software License
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests
Requires-Dist: grpcio
Requires-Dist: protobuf
Requires-Dist: grpcio-tools
Provides-Extra: dev
Requires-Dist: black ; extra == 'dev'
Requires-Dist: nbdev ; extra == 'dev'

# TorchServe Python Client

<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

## Install

``` sh
pip install torchserve_client
```

## Usage

Using `torchserve_client` is a breeze! It has support for both REST APIs
and gRPC APIs.

### REST Client

To make calls to REST endpoint, simply initialize a
[`TorchServeClientREST`](https://Ankur-singh.github.io/torchserve_client/rest.html#torchserveclientrest)
object as shown below:

``` python
from torchserve_client import TorchServeClientREST

# Initialize the REST TorchServeClient object
ts_client = TorchServeClientREST()
ts_client
```

    TorchServeClient(base_url=http://localhost, management_port=8081, inference_port=8080)

If you wish to customize the *base URL*, *management port*, or
*inference port* of your TorchServe server, you can pass them as
arguments during initialization:

``` python
from torchserve_client import TorchServeClientREST

# Customize the base URL, management port, and inference port
ts_client = TorchServeClientREST(base_url='http://your-torchserve-server.com', 
                             management_port=8081, inference_port=8080)
ts_client
```

    TorchServeClient(base_url=http://your-torchserve-server.com, management_port=8081, inference_port=8080)

### gRPC Client

To create a gRPC client, simply create a
[`TorchServeClientGRPC`](https://Ankur-singh.github.io/torchserve_client/grpc.html#torchserveclientgrpc)
object

``` python
from torchserve_client import TorchServeClientGRPC

# Initialize the gRPC TorchServeClient object
ts_client = TorchServeClientGRPC()
ts_client
```

    TorchServeClientGRPC(base_url=localhost, management_port=7071, inference_port=7070)

To customize base URL and default ports, pass them as arguments during
initialization

``` python
from torchserve_client import TorchServeClientGRPC

# Initialize the gRPC TorchServeClient object
ts_client = TorchServeClientGRPC(base_url='http://your-torchserve-server.com', 
                             management_port=7071, inference_port=7070)
ts_client
```

    TorchServeClientGRPC(base_url=your-torchserve-server.com, management_port=7071, inference_port=7070)

With these intuitive APIs at your disposal, you can harness the full
power of the Management and Inference API and take your application to
next level. Happy inferencing! ðŸš€ðŸ”¥
