"""Base SLM."""


import json
from openssm.core.adapter.base_adapter import BaseAdapter
from openssm.core.slm.abstract_slm import AbstractSLM
from openssm.core.adapter.abstract_adapter import AbstractAdapter
from openssm.utils.utils import Utils
from openssm.utils.logs import Logs
from openssm.core.prompts import Prompts


class BaseSLM(AbstractSLM):
    """Base SLM."""

    def __init__(self, adapter: AbstractAdapter = None):
        """
        self.conversations is initialized as a dictionary of conversations,
        where each conversation is a list of user inputs and model replies.
        """
        self._adapter = adapter
        self._conversations = {}

    @property
    def adapter(self) -> AbstractAdapter:
        """
        Return the previous assigned Adapter,
        or a default Adapter if none was assigned.
        """
        if self._adapter is None:
            self._adapter = BaseAdapter()
        return self._adapter

    @adapter.setter
    def adapter(self, adapter: AbstractAdapter):
        self._adapter = adapter

    @property
    def conversations(self) -> dict:
        """
        Return the previous assigned conversations,
        or an empty dictionary if none was assigned.
        """
        if self._conversations is None:
            self._conversations = {}
        return self._conversations

    @conversations.setter
    def conversations(self, conversations: dict):
        self._conversations = conversations

    @Utils.do_canonicalize_user_input('user_input')
    def discuss(self, user_input: list[dict], conversation_id: str = None) -> list[dict]:
        """
        Send user input to our language model and return the replies
        """
        # If conversation is new, start a new one, else continue from previous
        conversation = self.conversations.get(conversation_id, [])
        conversation.extend(user_input)

        replies = self._call_lm_api(conversation)
        replies = Utils.canonicalize_query_response(replies)

        # Save response to the conversation
        conversation.extend(replies)
        self.conversations[conversation_id] = conversation

        # Return the model's replies
        return replies

    def reset_memory(self):
        self.conversations = {}

    # pylint: disable=unused-argument
    def _call_lm_api(self, conversation: list[dict]) -> list[dict]:
        """
        Send conversation to the language modelâ€™s API
        and return the replies. Should be overridden by subclasses.
        """
        return [{"role": "assistant", "content": "Hello, how can I help you?"}]

    #
    # Helper functions for GPT-like completion models
    #
    @Logs.do_log_entry_and_exit()
    def _make_completion_prompt(self, conversation: list[dict]) -> str:
        # system = Prompts.get_module_prompt("openssm.core.slm.base_slm", "completion")
        system = Prompts.get_module_prompt(__name__, "completion")
        system = {"role": "system", "content": system}
        conversation = [system] + conversation
        prompt = str(conversation)
        # prompt = self._convert_conversation_to_string(conversation)
        # pylint: disable=pointless-string-statement
        """
        prompt = ("Complete this conversation with the response, "
                  "up to 2000 words (plus this prompt): "
                  "{'role': 'assistant', 'content': 'xxx'} format. "
                  "where 'xxx' is the response. "
                  "Make sure the entire response is valid JSON, xxx is "
                  "only a string, and no code of any kind, even if the "
                  "prompt has code. "
                  "Escape quotes with \\:\n"
                  f"{prompt}")
        """
        return prompt

    def _convert_conversation_to_string(self, conversation: list[dict]) -> str:
        list_conversation = []
        for item in conversation:
            role = item["role"].replace('"', '\\"')
            content = item["content"].replace('"', '\\"')
            list_conversation.append(
                f'{{"role": "{role}", "content": "{content}"}}'
            )
        return ", ".join(list_conversation)

    def _parse_llm_response(self, response) -> list[dict]:
        response = response.strip()

        if response.startswith('{') and not response.endswith('}'):
            response += '}'

        if response.endswith('}') and not response.startswith('{'):
            response += '{'

        if '{' not in response:
            response = json.dumps({"role": "assistant", "content": response})

        parsed_data = []
        start_indices = [i for i, c in enumerate(response) if c == '{']

        for start in start_indices:
            for end in range(start + 2, len(response) + 1):
                try:
                    json_str = response[start:end]
                    data = json.loads(json_str)
                    if isinstance(data, dict):
                        parsed_data.append(data)
                        break
                except json.JSONDecodeError:
                    continue

        return parsed_data

    def save(self, storage_dir: str):
        """Saves to the specified directory."""
        pass

    def load(self, storage_dir: str):
        """Loads from the specified directory."""
        pass


class PassthroughSLM(BaseSLM):
    """
    The PassthroughSLM is a barebones SLM that simply passes
    all queries to the adapter.
    """

    @Utils.do_canonicalize_user_input_and_query_response('user_input')
    def discuss(self, user_input: list[dict], conversation_id: str = None) -> list[dict]:
        """
        Pass through user input to the adapter and return the replies
        """
        # If conversation is new, start a new one, else continue from previous
        conversation = self.conversations.get(conversation_id, [])
        conversation.extend(user_input)

        replies = self.adapter.query(user_input, conversation_id)
        replies = Utils.canonicalize_query_response(replies)

        # Save response to the conversation
        conversation.extend(replies)
        self.conversations[conversation_id] = conversation

        # Return the model's replies
        return replies
