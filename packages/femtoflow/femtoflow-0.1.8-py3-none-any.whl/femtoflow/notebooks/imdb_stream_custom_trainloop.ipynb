{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "g0SLQIXOMOJt"
      },
      "source": [
        "# Pruning (Custom Trainloop) and Quantization on IMDB Sentiment Analysis with Streaming LSTM Model\n",
        "\n",
        "This notebook demonstrates the application of pruning and quantization techniques on a streaming LSTM model for the task of IMDB sentiment analysis. The key components and steps of the process are outlined below.\n",
        "\n",
        "## Overview\n",
        "\n",
        "- **Task:** IMDB Sentiment Analysis\n",
        "- **Model:** Streaming LSTM Model\n",
        "- **Layers to Prune:** RNN and Dense layers\n",
        "- **Pencil Size:** 4\n",
        "\n",
        "## Tutorial Flow\n",
        "\n",
        "1. **Pruning Using a Custom Training Loop:** This notebook showcases the process of pruning using a custom training loop. The goal is to sparsify the model while maintaining its performance.\n",
        "\n",
        "2. **Training and Sparsification in Non-Streaming Mode:** We initially train and sparsify the LSTM model in non-streaming mode. This allows us to obtain a set of sparse weights.\n",
        "\n",
        "3. **Conversion to Streaming Mode:** For TFLite conversion, we create a \"streaming version\" of the model with `batch_size=1` and `time_steps=1`. The sparse weights obtained from the non-streaming model are loaded onto this streaming version of the model.\n",
        "\n",
        "4. **TFLite Conversion:** The streaming version of the model, equipped with the sparse weights, is then converted to TFLite format for deployment on edge devices.\n",
        "\n",
        "5. Compiling TFLite model using [femtocrux](https://femtocrux.femtosense.ai/en/latest/)\n",
        "\n",
        "6. Generating Program Files from the Compiled Model using [femtodriverpub](https://github.com/femtosense/femtodriverpub) , and loading them onto SPU. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-sM9XwAtMOJw"
      },
      "source": [
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjq1SMdgMOJw",
        "outputId": "fb5efba4-672d-49b1-e788-fb8f2407e0aa",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# ! pip install femtoflow --quiet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fansCQYeMm3f"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0K9dk9IRMOJw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from tensorflow import keras\n",
        "import math\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from typing import List\n",
        "\n",
        "from femtoflow.quantization.quantize_tflite import TFLiteModelWrapper\n",
        "from femtoflow.sparsity.prune import PruneHelper\n",
        "from femtoflow.utils.plot import plot_prune_mask\n",
        "from femtoflow.utils.metrics import calculate_sparsity, get_gzipped_model_size\n",
        "\n",
        "import femtocrux\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" # Change 0 to the index of the de\n",
        "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
        "# os.environ['GH_PACKAGE_KEY']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(\"Found the following GPUs:\")\n",
        "    for gpu in gpus:\n",
        "        print(gpu)\n",
        "else:\n",
        "    print(\"No GPUs found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2dyVAElXMOJx"
      },
      "source": [
        "## Define Parameters\n",
        "\n",
        "### IMDB Data Parameters\n",
        "- `imdb_num_words`: Number of words to keep from the IMDB dataset based on frequency.\n",
        "- `imdb_skip_top_words`: Number of most frequently occurring words to skip.\n",
        "- `imdb_num_classes`: Number of classes for sentiment classification (positive/negative).\n",
        "- `imdb_max_seq_len`: Maximum sequence length for each review. Reviews with more words than this value will be discarded.\n",
        "\n",
        "### Model Parameters\n",
        "- `num_layers`: Number of layers in the LSTM model.\n",
        "- `hidden_dimension`: Dimension of the hidden state in the LSTM layers.\n",
        "- `embedding_dimension`: Dimension of the word embeddings.\n",
        "- `regularization`: L2 regularization strength for model weights.\n",
        "- `prob_output_name`: Name of the output layer for predicted probabilities.\n",
        "- `mask_input_name`: Name of the input layer for mask values.\n",
        "- `word_input_name`: Name of the input layer for word sequences.\n",
        "- `hidden_input_names`: List of input layer names for hidden states in each LSTM layer.\n",
        "- `hidden_output_names`: List of output layer names for hidden states in each LSTM layer.\n",
        "- `cell_input_names`: List of input layer names for cell states in each LSTM layer.\n",
        "- `cell_output_names`: List of output layer names for cell states in each LSTM layer.\n",
        "\n",
        "### Training Parameters\n",
        "- `batch_size`: Batch size for training.\n",
        "- `epochs`: Number of epochs for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTjJ5a8rMOJx"
      },
      "outputs": [],
      "source": [
        "# IMDB data parameters\n",
        "imdb_num_words = 2 ** 10 # Only these many words are kept\n",
        "imdb_skip_top_words = 2 # Skip this many most freqently occurring words\n",
        "imdb_num_classes = 2\n",
        "imdb_max_seq_len = 64 # Discard all data with more than this # of words\n",
        "    \n",
        "# Model parameters\n",
        "num_layers = 2\n",
        "hidden_dimension = 8\n",
        "embedding_dimension = 8\n",
        "regularization = 1e-5\n",
        "prob_output_name = 'prob'\n",
        "mask_input_name = 'mask'\n",
        "word_input_name = 'word'\n",
        "hidden_input_names = ['hidden_input_%d' % idx for idx in range(num_layers)]\n",
        "hidden_output_names = ['hidden_output_%d' % idx for idx in range(num_layers)]\n",
        "cell_input_names = ['cell_input_%d' % idx for idx in range(num_layers)]\n",
        "cell_output_names = ['cell_output_%d' % idx for idx in range(num_layers)]\n",
        "\n",
        "# Training parameters\n",
        "batch_size=32\n",
        "epochs=5\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uy2uajJWMOJy"
      },
      "source": [
        "## Define the Model\n",
        "\n",
        "The `sentiment_model` function defines the architecture of the LSTM model for sentiment analysis. The model consists of the following key components:\n",
        "- Embedding layer: Transforms one-hot encoded word inputs into dense embeddings.\n",
        "- LSTM layers: A sequence of LSTM layers for capturing temporal dependencies.\n",
        "- Final classification layer: A fully connected layer with a sigmoid activation function for binary classification.\n",
        "\n",
        "The function allows for two modes of operation:\n",
        "- Training mode: Variable batch size and time steps, along with a mask, are used for training the model.\n",
        "- Inference mode: A fixed batch size of 1 and time steps of 1 are used for inference, with no mask.\n",
        "\n",
        "The function returns a Keras model object with the specified architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WzuGI1AMOJy"
      },
      "outputs": [],
      "source": [
        "\n",
        "def sentiment_model(batch_size: int = None, timesteps: int = None, training: bool = True):\n",
        "    \"\"\"\n",
        "        Creates a model: embedding -> LSTMs -> FC -> sigmoid\n",
        "\n",
        "        For training, we use variable batch size and time steps, along with a mask.\n",
        "\n",
        "        For inference, we use a batch size = 1 and time steps = 1, with no mask.\n",
        "    \"\"\"\n",
        "    # regularizer = tf.keras.regularizers.L2(0.01)\n",
        "    \n",
        "    # Input layers: one-hot word encoding, hidden features (for future timesteps), initial state\n",
        "    initial_hiddens = [keras.layers.Input(\n",
        "        batch_size=batch_size,\n",
        "        shape=(hidden_dimension,),\n",
        "        name=hidden_input_names[idx]\n",
        "        ) for idx in range(num_layers)]\n",
        "    initial_cells = [keras.layers.Input(\n",
        "        batch_size=batch_size,\n",
        "        shape=(hidden_dimension,), \n",
        "        name=cell_input_names[idx]\n",
        "        ) for idx in range(num_layers)]\n",
        "    words_one_hot = keras.layers.Input(\n",
        "        batch_size=batch_size,\n",
        "        shape=(timesteps, imdb_num_words), \n",
        "        name=word_input_name\n",
        "    )\n",
        "    mask = keras.layers.Input(\n",
        "        batch_size=batch_size,\n",
        "        shape=(timesteps,),\n",
        "        name=mask_input_name\n",
        "    ) if training else None\n",
        "\n",
        "\n",
        "    # Learned embedding layer to map words to features.\n",
        "    # We use fully connected rather than keras.layers.Embedding because our model will have int8 inputs\n",
        "    embedding = keras.layers.Dense(\n",
        "        units=embedding_dimension,\n",
        "        use_bias=False,\n",
        "        activity_regularizer=None,\n",
        "        kernel_regularizer=None\n",
        "    )(words_one_hot)\n",
        "\n",
        "    # Reshape the inputs to (timesteps, inputs)\n",
        "    embedding = tf.keras.layers.Reshape(\n",
        "        target_shape=(\n",
        "            timesteps if timesteps is not None else -1, \n",
        "            embedding_dimension\n",
        "        )\n",
        "    )(embedding)\n",
        "\n",
        "    # LSTM layers with initial state\n",
        "    rnn_output = embedding\n",
        "    final_hiddens = []\n",
        "    final_cells = []\n",
        "    for layer_idx, (initial_hidden, initial_cell) in enumerate(zip(initial_hiddens, initial_cells)):\n",
        "        rnn_output, final_hidden, final_cell = tf.keras.layers.RNN(\n",
        "                tf.keras.layers.LSTMCell(\n",
        "                    units=hidden_dimension,\n",
        "                    kernel_regularizer=None\n",
        "                    ),\n",
        "                return_sequences = True,\n",
        "                return_state = True,\n",
        "                unroll = not training\n",
        "                )(rnn_output, mask=mask, initial_state=[initial_hidden, initial_cell])\n",
        "\n",
        "        # Pass the hidden and cell states into identity layers, to rename them\n",
        "        final_hiddens.append(keras.layers.Layer(name=hidden_output_names[layer_idx])(final_hidden))\n",
        "        final_cells.append(keras.layers.Layer(name=cell_output_names[layer_idx])(final_cell))\n",
        "\n",
        "    # Final classification layer\n",
        "    final_rnn_output = rnn_output[:, -1]\n",
        "    prob = keras.layers.Dense(\n",
        "            units=imdb_num_classes, \n",
        "            activation='sigmoid',\n",
        "            name=prob_output_name\n",
        "            )(final_rnn_output)\n",
        "\n",
        "    # Choose the model inputs/outputs for training and inference mode\n",
        "    inputs=[words_one_hot] + initial_hiddens + initial_cells\n",
        "    outputs={\n",
        "        prob_output_name: prob\n",
        "    }\n",
        "    if training:\n",
        "        inputs += [mask,]\n",
        "    else:\n",
        "        for idx, (hidden_output_name, cell_output_name) in enumerate(zip(hidden_output_names, cell_output_names)):\n",
        "            outputs[hidden_output_name] = final_hiddens[idx]\n",
        "            outputs[cell_output_name] = final_cells[idx]\n",
        "\n",
        "    return keras.models.Model(\n",
        "            inputs=inputs,\n",
        "            outputs=outputs\n",
        "    )\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rwogsXEoMOJz"
      },
      "source": [
        "## Define Utility Functions\n",
        "\n",
        "This section defines utility functions used for data processing, sequence prediction, and model conversion:\n",
        "\n",
        "- `prepare_inputs`: Prepares the input sequences for the model in a dictionary format understood by the TensorFlow model. This function performs padding, one-hot encoding, and initializes hidden states.\n",
        "\n",
        "- `__process_sequences__`: Runs the model on sequence data and processes the inputs and initial states as separate time slices. This function can be used as a data generator for TensorFlow to TFLite conversion or as an inference engine.\n",
        "\n",
        "- `__batch_process_sequences__`: A wrapper function for `__process_sequences__` that performs batch processing of sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVFymv8XMOJ0"
      },
      "outputs": [],
      "source": [
        "def prepare_inputs(x: List[int], y: List[int] = None):\n",
        "    \"\"\"\n",
        "    Process inputs from dataloader, in a dictionary format\n",
        "    understood by the Tensorflow Model.\n",
        "    \"\"\"\n",
        "\n",
        "    have_y = y is not None\n",
        "\n",
        "    # Preprocessing: get a batch and pad to a common length\n",
        "    max_seq_len = max(len(seq) for seq in x)\n",
        "    x_words = np.zeros((len(x), max_seq_len))\n",
        "    x_words.fill(np.nan)\n",
        "    for x_idx, seq in enumerate(x):\n",
        "        x_words[x_idx, :len(seq)] = seq\n",
        "    mask = ~np.isnan(x_words)\n",
        "    x_words[~mask] = 0\n",
        "\n",
        "    # Convert words and labels to one-hot\n",
        "    x_one_hot = keras.utils.to_categorical(x_words, imdb_num_words)\n",
        "    y_one_hot = keras.utils.to_categorical(y, imdb_num_classes) if have_y else None\n",
        "\n",
        "    # Initialize the hidden state to zeros\n",
        "    initial_hiddens = {\n",
        "            hidden_input_names[idx]: np.zeros((len(x), hidden_dimension), dtype=np.float32) for idx in range(num_layers)\n",
        "            }\n",
        "    initial_cells = {\n",
        "            cell_input_names[idx]: np.zeros_like(initial_hiddens[hidden_input_names[idx]]) for idx in range(num_layers)\n",
        "            }\n",
        "\n",
        "    # Collect all the inputs\n",
        "    inputs = {\n",
        "        mask_input_name: mask,\n",
        "        word_input_name: x_one_hot\n",
        "    }\n",
        "    inputs.update(initial_hiddens)\n",
        "    inputs.update(initial_cells)\n",
        "\n",
        "    return (inputs, y_one_hot) if have_y else inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95OHFm8oMOJ0"
      },
      "outputs": [],
      "source": [
        "def __process_sequences__(predict_fun, x, yield_inputs: bool = False) -> List[np.array]:\n",
        "    \"\"\"\n",
        "        Runs the model on sequence data, then yields the inputs and initial \n",
        "        state as separate time slices.\n",
        "\n",
        "        This can be used as a data generator for TF -> TFLite conversion, if y is None,\n",
        "        or as an inference engine, if y is not None.\n",
        "\n",
        "        Arguments:\n",
        "            predict_fun: Runs prediction on the inputs. Same signature as predict() method of \n",
        "                keras.Model.\n",
        "            x: Input data\n",
        "            yield_intputs: If true, acts as a generator yielding model inputs. Used for TFLite quantization.\n",
        "    \"\"\"\n",
        "    # Process each sequence individually\n",
        "    sequence_inputs = prepare_inputs(x)\n",
        "    batch_size, num_timesteps, num_words = sequence_inputs[word_input_name].shape\n",
        "    print(\"Input stats:\")\n",
        "    print(\"\\tNum sequences: %d\" % batch_size)\n",
        "    print(\"\\tMax sequence length: %d\" % num_timesteps)\n",
        "    print(\"\\tNum words: %d\" % num_words)\n",
        "    print(\"\\tYielding inputs: %d\" % yield_inputs)\n",
        "    for batch_idx in range(batch_size):\n",
        "        # Process each time step individually\n",
        "        # For the first iteration, use the sequence input hidden state\n",
        "        print(\"Generating sequence %d / %d...\" % (batch_idx, batch_size))\n",
        "        initial_hidden = [sequence_inputs[hidden_input_name][None, batch_idx] for hidden_input_name in hidden_input_names]\n",
        "        initial_cell = [sequence_inputs[cell_input_name][None, batch_idx] for cell_input_name in cell_input_names]\n",
        "        for timestep in range(num_timesteps):\n",
        "\n",
        "            # Quit if the sequence is over\n",
        "            mask_seq = sequence_inputs[mask_input_name][batch_idx]\n",
        "            if not mask_seq[timestep]:\n",
        "                break\n",
        "\n",
        "            # Extract the inputs for this time slice\n",
        "            inputs = {\n",
        "                word_input_name: sequence_inputs[word_input_name][None, None, batch_idx, timestep],\n",
        "            }\n",
        "            for hidden_name, hidden_val in zip(hidden_input_names, initial_hidden):\n",
        "                inputs[hidden_name] = hidden_val\n",
        "            for cell_name, cell_val in zip(cell_input_names, initial_cell):\n",
        "                inputs[cell_name] = cell_val\n",
        "\n",
        "            # In quantization mode, return inputs to the TF -> TFlite converter\n",
        "            if yield_inputs: \n",
        "                yield inputs\n",
        "\n",
        "            # Proces the time step\n",
        "            outputs = predict_fun(inputs)\n",
        "\n",
        "            # Record the next hidden state\n",
        "            initial_hidden = [outputs[hidden_output_name] for hidden_output_name in hidden_output_names]\n",
        "            initial_cell = [outputs[cell_output_name] for cell_output_name in cell_output_names]\n",
        "\n",
        "        # Collect the prediction at the end of each sequence\n",
        "        if not yield_inputs:\n",
        "            yield outputs[prob_output_name]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_TSYCZzMOJ0"
      },
      "outputs": [],
      "source": [
        "def __batch_process_sequences__(predict_fun, x: List) -> List:\n",
        "    \"\"\"\n",
        "        Wrapper for process_sequences, using batch processing rather than a generator.\n",
        "    \"\"\"\n",
        "    yield_inputs = False\n",
        "    return [x for x in __process_sequences__(predict_fun, x, yield_inputs=yield_inputs)]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BX9vMejjMOJ1"
      },
      "source": [
        "## Download IMDB Sentiment Dataset\n",
        "\n",
        "The IMDB Sentiment Dataset is downloaded and preprocessed to be used for model training, evaluation, and quantization calibration. The dataset consists of movie reviews labeled as either positive or negative sentiment.\n",
        "\n",
        "We use the `imdb.load_data` function from the Keras datasets module to load the data. The dataset is split into training and test sets, with the following considerations:\n",
        "- Only a subset of the most frequent words (`imdb_num_words`) is considered, with the top `imdb_skip_top_words` being skipped.\n",
        "- Reviews with more than `imdb_max_seq_len` words are truncated.\n",
        "- The test set is further abbreviated for faster processing during evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWQP78OtMOJ1",
        "outputId": "d11287f5-f0c7-4912-d77e-cd2e20db2ee6"
      },
      "outputs": [],
      "source": [
        "import keras.datasets.imdb as imdb\n",
        "\n",
        "# Get MNIST train and test splits\n",
        "# From the training split, we will also take an 'eval' set \n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
        "    num_words=imdb_num_words,\n",
        "    skip_top=imdb_skip_top_words,\n",
        "    maxlen=imdb_max_seq_len\n",
        ")\n",
        "\n",
        "# Abbreviate the test set, for speed \n",
        "num_test_samples = int(1e3)\n",
        "x_test = x_test[:num_test_samples]\n",
        "y_test = y_test[:num_test_samples]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kF8WHzaTMOJ1"
      },
      "source": [
        "## Train Non-Streaming Version of the Model\n",
        "\n",
        "### Training Loop\n",
        "\n",
        "To train the non-streaming version of the LSTM model, we first create an instance of the model using the `sentiment_model` function with `training=True`. The model is then compiled with the specified optimizer, loss function, and evaluation metrics.\n",
        "\n",
        "The training loop runs for a specified number of epochs. In each epoch, the training data is divided into batches, and the model is trained on each batch using the `train_on_batch` function. The training process involves preprocessing the batch data and updating the model weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7QNT4h-MOJ1",
        "outputId": "b69d57fc-6ffc-4230-f13b-ef17e5acff89"
      },
      "outputs": [],
      "source": [
        "# Create an network: embedding -> LSTM -> FC\n",
        "train_model = sentiment_model(training=True)\n",
        "\n",
        "# Train the model\n",
        "optimizer='adam'\n",
        "loss_fn='categorical_crossentropy'\n",
        "metrics=['accuracy']\n",
        "train_model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "for epoch in range(epochs):\n",
        "    print(\"Epoch %d / %d ...\" % (epoch, epochs))\n",
        "\n",
        "    rng = np.random.default_rng()\n",
        "    num_batches = math.ceil(len(x_train) / batch_size)\n",
        "    for batch_idx in range(num_batches):\n",
        "        print(\"Batch %d / %d ...\" % (batch_idx, num_batches))\n",
        "\n",
        "        # Preprocess batch data\n",
        "        batch_inds = rng.choice(len(x_train), size=batch_size) \n",
        "        x_batch = x_train[batch_inds]\n",
        "        y_batch = y_train[batch_inds] \n",
        "        inputs, y_batch_one_hot = prepare_inputs(x_batch, y_batch)\n",
        "\n",
        "        # Train on this batch\n",
        "        train_model.train_on_batch(inputs, y_batch_one_hot)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9cg9cSG5MOJ2"
      },
      "source": [
        "### Evaluation of Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsREZ6UKMOJ2",
        "outputId": "93b056fa-a229-4f30-b1ba-41c147075888"
      },
      "outputs": [],
      "source": [
        "# Evaluate on each split\n",
        "train_inputs, y_train_one_hot = prepare_inputs(x_train, y_train)\n",
        "test_inputs, y_test_one_hot = prepare_inputs(x_test, y_test)\n",
        "tf_train_loss, tf_train_accuracy  = train_model.evaluate(train_inputs, y_train_one_hot, batch_size=batch_size)\n",
        "tf_test_loss, tf_test_accuracy  = train_model.evaluate(test_inputs, y_test_one_hot, batch_size=batch_size)\n",
        "print(\"Tensorflow loss:\")\n",
        "print(\"\\tTrain: %f\" % tf_train_loss)\n",
        "print(\"\\tTest: %f\" % tf_test_loss)\n",
        "print(\"Tensorflow accuracy:\")\n",
        "print(\"\\tTrain: %f\" % tf_train_accuracy)\n",
        "print(\"\\tTest: %f\" % tf_test_accuracy)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q1RyduAAMOJ2"
      },
      "source": [
        "## Sparsify Non-Streaming Model Using Custom Training Loop\n",
        "\n",
        "### Define the `prune_helper = PruneHelper()` Class\n",
        "\n",
        "We start by creating a TensorFlow dataset for training and testing, and define the `PruneHelper` class. The `PruneHelper` class is used to apply pruning to specific layers of the model during training. It does this by wrapping the specified Keras layers with `tfmot.sparsity.keras.pruning_wrapper.prune_low_magnitude`, which enables the layers to be sparsified during training.\n",
        "\n",
        "The `PruneHelper` class is configured with parameters such as `pencil_size`, `pencil_pooling_type`, and `prune_scheduler`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYCeju2wMOJ3"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, y_train_one_hot)).batch(batch_size, drop_remainder=True)\n",
        "test_dataset  = tf.data.Dataset.from_tensor_slices((test_inputs, y_test_one_hot)).batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5gpjkrdMOJ3"
      },
      "outputs": [],
      "source": [
        "pencil_size = 4\n",
        "pencil_pooling_type = 'AVG'\n",
        "prune_scheduler = 'poly_decay'  # 'constant' # 'poly_decay'\n",
        "prune_helper = PruneHelper(pencil_size=pencil_size,\n",
        "                             pencil_pooling_type=pencil_pooling_type,\n",
        "                             prune_scheduler=prune_scheduler)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UddKFVaMMOJ3"
      },
      "source": [
        "### Create `model_to_prune` with Pruning Wrappers\n",
        "\n",
        "The `model_to_prune` is created by applying pruning wrappers to the layers we want to sparsify in the original `train_model` using the `PruneHelper` class. Layers to be pruned include dense and RNN layers. Pruning parameters such as `initial_sparsity`, `final_sparsity`, `begin_step`, `end_step`, `prune_frequency`, and `power` are defined.\n",
        "\n",
        "The `model_to_prune` is returned with the specified layers wrapped with `tfmot.sparsity.pruning_wrapper.prune_low_magnitude`, allowing for sparsity to be induced during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0uSda87MOJ3",
        "outputId": "b9e7a327-c0c0-4b3a-bf27-b5683cc1b8d8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Define additional parameters for pruning\n",
        "\"\"\"\n",
        "layers_to_prune = [tf.keras.layers.Dense, tf.keras.layers.RNN] # Layers we want to prune \n",
        "initial_sparsity = 0.2\n",
        "final_sparsity = 0.6\n",
        "begin_step = 0\n",
        "end_step =  len(train_dataset)*epochs\n",
        "prune_frequency = 100\n",
        "power = 3\n",
        "\n",
        "model_to_prune = prune_helper(model=train_model,\n",
        "                                layers_to_prune=layers_to_prune,\n",
        "                                initial_sparsity=initial_sparsity,\n",
        "                                final_sparsity=final_sparsity,\n",
        "                                begin_step=begin_step,\n",
        "                                end_step=end_step,\n",
        "                                prune_frequency=prune_frequency,\n",
        "                                power=power)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lD-ArLq1MOJ3",
        "outputId": "3623acf8-c614-45d5-b27e-de6e59698b3b"
      },
      "outputs": [],
      "source": [
        "model_to_prune.layers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xDWu1cVIMOJ4"
      },
      "source": [
        "### Perform Training with Sparsity on `model_to_prune` Using a Custom Training Loop\n",
        "\n",
        "We define a custom training loop `custom_sparsity_train_loop` to train the model with sparsity. The loop includes calls to `tfmot.sparsity.keras.UpdatePruningStep()` at specific points to induce sparsity during training. The loop runs for a specified number of epochs, and pruning is applied to the model during each epoch.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U2dI6Kq5MOJ4"
      },
      "source": [
        "### Define the A Custom Sparsity-Train Loop\n",
        "\n",
        "Here, we want to define a custom training Loop, and Perform Pruning within this Custom Training Loop.\n",
        "Reference: https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEaChD-wMOJ4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_sparsity_train_loop(model_to_prune, \n",
        "                                optimizer, \n",
        "                                metrics, \n",
        "                                loss_fn, \n",
        "                                train_dataset, \n",
        "                                val_dataset=None,\n",
        "                                num_epochs=2):\n",
        "    \"\"\"\n",
        "    Custom Training Loop, with instances of tfmot.sparsity.keras.UpdatePruningStep() \n",
        "    called in specific points of the training loop (marked 1-4) to induce sparsity.\n",
        "    Reference: https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md\n",
        "    \"\"\"\n",
        "    model_to_prune.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "    \"\"\"\n",
        "    1) Define prune_step and attach Model to prune_step callback\n",
        "    \"\"\"\n",
        "    prune_step = tfmot.sparsity.keras.UpdatePruningStep()\n",
        "    prune_step.set_model(model_to_prune)\n",
        "\n",
        "    \"\"\"\n",
        "    2) prune_step.on_train_begin() call\n",
        "    \"\"\"\n",
        "    prune_step.on_train_begin() # call bac\n",
        "    for epoch in range(0, num_epochs):\n",
        "        print(f\"Processing epoch {epoch}\")\n",
        "        for batch_id, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "            \n",
        "            \"\"\"\n",
        "            3) prune_step.on_train_batch_begin() call\n",
        "            \"\"\"\n",
        "            prune_step.on_train_batch_begin(batch=-1) # run pruning callback\n",
        "\n",
        "            model_to_prune.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "        \"\"\"\n",
        "        4) prune_step.on_epoch_end() call\n",
        "        \"\"\"\n",
        "        prune_step.on_epoch_end(batch=-1) # run pruning callback\n",
        "\n",
        "    if val_dataset:\n",
        "        _, val_acc = model_to_prune.evaluate(val_dataset, verbose=0)\n",
        "        print(f\"Validation Accuracy Epoch {epoch}: {val_acc}\")\n",
        "\n",
        "    return model_to_prune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH-iUdGbMOJ4",
        "outputId": "a098c2db-2b2f-418e-fa26-947a507a684e"
      },
      "outputs": [],
      "source": [
        "model_pruned = custom_sparsity_train_loop(model_to_prune=model_to_prune,\n",
        "                                          optimizer=optimizer,\n",
        "                                          metrics=metrics, \n",
        "                                            loss_fn=loss_fn, \n",
        "                                            train_dataset=train_dataset, \n",
        "                                            val_dataset=test_dataset,\n",
        "                                            num_epochs=epochs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NNWhCtR4MOJ4"
      },
      "source": [
        "### Apply `tfmot.sparsity.keras.strip_pruning()` to Remove Sparse Layer Wrappers\n",
        "\n",
        "The pruned model (`model_to_prune`) contains a `tfmot.sparsity.keras.prune_low_magnitude()` wrapper around the layers that were sparsified during training. To finalize the pruning process, we use the `strip_pruning()` function, which removes the pruning wrappers and returns the pruned model with the sparse weights. The resulting `model_pruned_stripped` contains the final pruning mask applied to the weights, which represents the actual sparsity achieved by the pruning process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qCLIn7zMOJ4"
      },
      "outputs": [],
      "source": [
        "model_pruned_stripped = tfmot.sparsity.keras.strip_pruning(model_pruned)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2CdR5tUSMOJ5"
      },
      "source": [
        "### Evaluate Pruned Non-Streaming Model Accuracy\n",
        "\n",
        "After the pruning process, it is important to evaluate the performance of the pruned model (`model_pruned`) on both the training and test datasets. We measure the loss and accuracy of the pruned model and report the results. This helps us understand how the pruning process has affected the model's accuracy and generalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cm82aoIwMOJ5",
        "outputId": "b850bec7-e305-4be8-df16-b930ed60ef4f"
      },
      "outputs": [],
      "source": [
        "tf_prune_train_loss, tf_prune_train_accuracy  = model_pruned.evaluate(train_dataset)\n",
        "tf_prune_test_loss, tf_prune_test_accuracy  = model_pruned.evaluate(test_dataset)\n",
        "print(\"Tensorflow-Pruned loss:\")\n",
        "print(\"\\tTrain: %f\" % tf_prune_train_loss)\n",
        "print(\"\\tTest: %f\" % tf_prune_test_loss)\n",
        "print(\"Tensorflow-Pruned accuracy:\")\n",
        "print(\"\\tTrain: %f\" % tf_prune_train_accuracy)\n",
        "print(\"\\tTest: %f\" % tf_prune_test_accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElAMIKdMOJ5"
      },
      "source": [
        "### Visualize Pruned Weights\n",
        "Visualizing the pruned weights of the model can provide insights into the sparsity pattern and the distribution of non-zero weights in the layers. We use the `plot_prune_mask` function to create visualizations of the pruned weights for each layer in the model. The visualizations are saved as images for further analysis and inspection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDFcb448MOJ5",
        "outputId": "6dff9220-ea1d-47cb-a746-74e648f03ca5"
      },
      "outputs": [],
      "source": [
        "trainable_weights_dict = {weight.name: weight.numpy() for weight in model_pruned_stripped.trainable_weights}\n",
        "trainable_weights_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ET-hFfiuMOJ5",
        "outputId": "6f0d7baf-e505-424b-a6ed-4b9f97f5535f"
      },
      "outputs": [],
      "source": [
        "\n",
        "base_path = 'imdb_lstm_dense'\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "max_len_axis = 24\n",
        "for layer_name, layer in trainable_weights_dict.items():\n",
        "    title = f\"{layer_name}-shape-opxip-{layer.T.shape}\"\n",
        "    save_path = f\"{base_path}/{layer_name.replace('/', '-')}-prune-mask.png\"\n",
        "    # print(title)\n",
        "    plot_prune_mask(data=layer, axis_stride=pencil_size, max_xlen=max_len_axis, max_ylen=max_len_axis, title=title, save_path=save_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r85zccC6MOJ5"
      },
      "source": [
        "### Calculate Sparsity Metrics\n",
        "\n",
        "Sparsity refers to the proportion of zero-valued elements in the model's weights. After the pruning process, it is important to calculate the sparsity for each layer in the pruned model to understand the level of sparsity achieved.\n",
        "\n",
        "We use the `calculate_sparsity` function to calculate the sparsity for each layer's weights. The results are stored in the `sparsity_dict` dictionary, where the keys represent the layer names and the values represent the calculated sparsity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uoj5N-3yMOJ5",
        "outputId": "1f086099-11c1-4e2e-f93c-9449faeef2d5"
      },
      "outputs": [],
      "source": [
        "sparsity_dict = {}\n",
        "for layer_name, layer_weight in trainable_weights_dict.items():\n",
        "    sparsity_dict[layer_name] = calculate_sparsity(layer_weight)\n",
        "print('sparsity_dict', sparsity_dict)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qZYJ-ZbUMOJ5"
      },
      "source": [
        "## Define the Streaming Inference Model and Load Pruned Weights\n",
        "\n",
        "For real-time or streaming inference, we need to create a version of the model that is capable of processing data in single-batch and single-time-step increments. This model is often referred to as the streaming model.\n",
        "\n",
        "### Create Streaming Inference Model\n",
        "\n",
        "We use the `sentiment_model` function to create an inference version of the model. We set the `batch_size` and `timesteps` to 1 to allow for single-batch and single-time-step processing. We also set `training=False` to indicate that this is an inference model.\n",
        "\n",
        "### Load Pruned Weights into Streaming Model\n",
        "\n",
        "To use the sparsity achieved during training, we load the pruned weights from the `model_pruned_stripped` model (the pruned non-streaming model) into the streaming inference model. This allows the streaming model to benefit from the sparsity and compression achieved through the pruning process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWWTDq0pMOJ5",
        "outputId": "3f2e10df-29dd-4cc6-ff70-a7684d85e27b"
      },
      "outputs": [],
      "source": [
        "# Create an inference version of the model, for single batch size and time steps (streaming model)\n",
        "inference_model = sentiment_model(batch_size=1, timesteps=1, training=False)\n",
        "print(\"Setting Weights of Streaming Inference Model to the Sparsified Trained Weights of the Non-Streaming Model\")\n",
        "inference_model.set_weights(model_pruned_stripped.get_weights())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-38Dbe7iMOJ6"
      },
      "source": [
        "## Quantize the Pruned Streaming Model Using TFLite\n",
        "\n",
        "Quantization is a technique used to reduce the memory footprint and improve the computational efficiency of a model by converting the weights and activations from floating-point representation to fixed-point representation. In this section, we will quantize the pruned streaming model using TensorFlow Lite (TFLite).\n",
        "\n",
        "### Quantization Modes\n",
        "\n",
        "We can choose between two quantization modes for the pruned streaming model:\n",
        "\n",
        "1. **8-bit Quantization (`quantize_mode = '8x8'`)**: This mode quantizes both the weights and activations to 8-bit integer values (Int8). It significantly reduces the model size and offers good computational efficiency while maintaining reasonable accuracy.\n",
        "\n",
        "2. **Hybrid Quantization (`quantize_mode = '8x16'`)**: This mode quantizes the weights to 8-bit integer values (Int8) and the activations to 16-bit integer values (Int16). It offers higher precision for activations compared to 8-bit quantization, making it suitable for use cases where higher accuracy is required.\n",
        "\n",
        "The choice of quantization mode depends on the specific requirements of the application, such as the acceptable level of accuracy and the computational resources available on the target deployment platform.\n",
        "\n",
        "### Quantize Using `TFLiteModelWrapper()` Class\n",
        "\n",
        "To perform quantization, we will use the `TFLiteModelWrapper()` class. This class provides a convenient interface for converting a TensorFlow model to a quantized TFLite model. We will configure the class to perform the selected quantization mode and apply it to the pruned streaming model.\n",
        "\n",
        "We first define a `keras_predict_fun` function to perform prediction using the inference model. Next, we create a `representative_dataset` function that generates the representative dataset for quantization calibration. This dataset is generated using the `__process_sequences__` function and reflects the typical input data distribution.\n",
        "\n",
        "We then set the quantization mode (`quantize_mode`) to either `'8x8'` or `'8x16'` based on the desired quantization type. We provide the model, representative dataset, and quantization mode to the `TFLiteModelWrapper()` class and specify the save path for the quantized TFLite model (`tflite_save_path`).\n",
        "\n",
        "The `model_tflite` is the quantized TFLite model that is optimized for deployment on resource-constrained devices. It benefits from both the sparsity achieved during pruning and the reduced memory footprint achieved through quantization. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXJHi_X9MOJ6"
      },
      "outputs": [],
      "source": [
        "# Truncate the datasets, for speed\n",
        "num_quantization_sequences = 16\n",
        "x_quant = x_train[:num_quantization_sequences]\n",
        "y_quant = y_train[:num_quantization_sequences]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb7XbEdvMOJ6"
      },
      "outputs": [],
      "source": [
        "# Preprocess inference inputs\n",
        "quant_inputs = prepare_inputs(x_quant)\n",
        "test_inputs = prepare_inputs(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3CCzHaRMOJ6",
        "outputId": "cc50173e-4419-4bbf-ee97-5d004fbfba94"
      },
      "outputs": [],
      "source": [
        "keras_predict_fun = lambda inputs: inference_model.predict(inputs, verbose=False)\n",
        "representative_dataset = lambda: __process_sequences__(keras_predict_fun, x_quant, yield_inputs=True) \n",
        "\n",
        "quantize_mode = '8x8' # or '8x16' \n",
        "\n",
        "tflite_save_path = 'tflite_imdb.tflite'\n",
        "model_tflite = TFLiteModelWrapper(quantize_mode=quantize_mode,\n",
        "                                  model=inference_model, \n",
        "                                  representative_dataset=representative_dataset, \n",
        "                                  tflite_save_path=tflite_save_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rPTpmWj9MOJ6"
      },
      "source": [
        "### Evaluate Performance of TFLite Streaming Model (Pruned and Quantized)\n",
        "\n",
        "In this section, we assess the performance of the pruned and quantized TFLite streaming model generated in the previous steps. To achieve this, we measure the model's classification accuracy on the test dataset.\n",
        "\n",
        "We define a function `tflite_predict_fun` as a lambda function to perform predictions using the TFLite model `model_tflite`. We process the test dataset using the `__batch_process_sequences__` function, which performs batch processing of sequences and returns the predictions.\n",
        "\n",
        "The `classification_accuracy` function calculates the accuracy by comparing the predicted class labels (obtained by taking the argmax of the logits) with the ground truth labels. The computed accuracy reflects the effectiveness of the pruned and quantized TFLite streaming model in performing sentiment analysis.\n",
        "\n",
        "Finally, we print the TFLite model's classification accuracy, which provides insight into the model's performance on the IMDB sentiment analysis task. It is important to note that the model has undergone both pruning and quantization, optimizing it for deployment on resource-constrained devices while maintaining reasonable accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOKYBGIIMOJ6",
        "outputId": "94cef293-0074-43f2-888f-81a93f002f4f"
      },
      "outputs": [],
      "source": [
        "tflite_predict_fun = lambda x: model_tflite(x) \n",
        "\n",
        "# Measure the TFLite model's accuracy\n",
        "tflite_labels = []\n",
        "reference_tflite_preds = []\n",
        "print(\"Measuring TFLite accuracy...\")\n",
        "tflite_preds = __batch_process_sequences__(\n",
        "    tflite_predict_fun, \n",
        "    x_test\n",
        ")\n",
        "\n",
        "def classification_accuracy(logits: np.array, labels: np.array):\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    return np.mean(labels == predictions)\n",
        "\n",
        "tflite_accuracy = classification_accuracy(tflite_preds, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw2BPcO_MOJ6",
        "outputId": "02f9d82d-6af3-42e0-b174-2e47b4f81bf6"
      },
      "outputs": [],
      "source": [
        "print(\"TFLite reference accuracy: %f\" % tflite_accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqx-B6DmMOJ7"
      },
      "source": [
        "### Compare Model Sizes: Baseline, Pruned, and Pruned/Quantized\n",
        "\n",
        "In this section, we evaluate and compare the sizes of different versions of the model: the baseline model, the pruned model, and the pruned and quantized TFLite model. This comparison helps us understand the impact of pruning and quantization on the model's memory footprint, which is critical for deployment on resource-constrained devices.\n",
        "\n",
        "We start by saving the baseline model and the pruned model (with pruning wrappers stripped) to temporary files. Next, we retrieve the path of the pruned and quantized TFLite model, which was generated in previous steps.\n",
        "\n",
        "To calculate and compare the model sizes, we use the `get_gzipped_model_size` function, which computes the size of a gzipped model file. We print the sizes of the gzipped baseline Keras model, the gzipped pruned Keras model, and the gzipped pruned/quantized TFLite model.\n",
        "\n",
        "The results provide insights into the effectiveness of pruning and quantization in reducing the model's memory footprint while maintaining its performance on the IMDB sentiment analysis task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIk4LyelMOJ7",
        "outputId": "1c01a4fe-3ce0-4f16-e046-0b7772c447cc"
      },
      "outputs": [],
      "source": [
        "\n",
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(train_model, keras_file, include_optimizer=False)\n",
        "print('Saved baseline model to:', keras_file)\n",
        "\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model_pruned_stripped, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)\n",
        "\n",
        "pruned_tflite_file = tflite_save_path\n",
        "print(\"TFLite File was generated at:\", pruned_tflite_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbzy66kXMOJ7",
        "outputId": "cc1b766f-5200-49a3-90f5-b0fd94cea555"
      },
      "outputs": [],
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned/Quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compiling TFLite model using [femtocrux](https://femtocrux.femtosense.ai/en/latest/) and generating Memory Image BitFile\n",
        "\n",
        "Next, we will compile the generated TFLite model with Femtocrux. Compiling the model using Femtocrux is a necessary step in deploying the Tensorflow model on Femtosense's SPU.\n",
        "\n",
        "We will need to have Docker installed and instantiate a CompilerClient. This will allow us to make API calls to the Femtosense's compiler. We can call the `compile` method of femtocrux to produce the `Memory Image bitfile`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from femtocrux import CompilerClient, TFLiteModel\n",
        "client = CompilerClient()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flatbuffer = model_tflite.instance.flatbuffer\n",
        "signature_name = model_tflite.instance.signature_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bitstream = client.compile(    \n",
        "    TFLiteModel(flatbuffer=flatbuffer, signature_name=signature_name)\n",
        ")\n",
        "# Write to a file for later use\n",
        "with open('my_bitfile.zip', 'wb') as f: \n",
        "    f.write(bitstream)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Program Files, and loading onto SPU. \n",
        "The memory image bitfile zip can then be converted to Program Files using [femtodriverpub](https://github.com/femtosense/femtodriverpub).\n",
        "\n",
        "Once generated, these Program Files can be transferred to an SD card, which can then be inserted into Femtosense's SPU. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Install `Femtodriverpub`\n",
        "##### Step One - Clone femtodriverpub from Github and Install it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! git clone https://github.com/femtosense/femtodriverpub.git; cd femtodriverpub; pip install -e ."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Unzip the `my_bitfile.zip` zip folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! rm -rf 'my_bitfile'\n",
        "! unzip 'my_bitfile.zip' -d 'my_bitfile'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generate the Program Files to load onto SPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! python femtodriverpub/femtodriverpub/run/sd_from_femtocrux.py 'my_bitfile'\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Program Files should be generated at `apb_records` folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! ls 'apb_records'\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The contents inside `apb_records` can be loaded onto a SD card, which can then be inserted onto the SPU! Congratulations, your model is now ready to deploy on the SPU!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "85b2d2b1704bae26dccb80a5cc58eb80df19b6308dfb080f4439b2655dd4e2ef"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
