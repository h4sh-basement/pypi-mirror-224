{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "u2gWf-rDZgsB"
      },
      "source": [
        "# Simple Quantization Example\n",
        "\n",
        "In this example, we demonstrate a simple quantization workflow using `femtoflow`. Quantization is a technique used to reduce the memory footprint and improve the computational efficiency of a model by converting the weights and activations from floating-point representation to fixed-point representation.\n",
        "\n",
        "## Considerations\n",
        "\n",
        "- Quantization may introduce a slight drop in model accuracy due to the reduced precision. The representative dataset and calibration process help minimize the accuracy loss.\n",
        "\n",
        "- Different quantization modes (e.g., `8x8`, `8x16`) offer trade-offs between model size, accuracy, and performance. Choosing the right quantization mode depends on the specific use case and deployment requirements.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MwsWqYM0ZgsF"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pX-HG5e3ZgsF",
        "outputId": "63c0a428-5675-4093-d61f-5a533c6de54a",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# ! pip install femtoflow --quiet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EwYbZlNtZgsG"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbMCayUbZgsH"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "from femtoflow.quantization.quantize_tflite import TFLiteModelWrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ui2aEYT5ZgsH"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5j5P9FtkZgsI"
      },
      "source": [
        "## MNIST Dataset Download\n",
        "\n",
        "We will use the MNIST dataset for this example. The dataset consists of grayscale images of handwritten digits (0-9) and corresponding labels. Each image has a size of 28x28 pixels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jm8o-i4ZgsI",
        "outputId": "5e17e3ba-4c12-464d-e0de-98fae7f0e19f"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(BATCH_SIZE, drop_remainder=True)\n",
        "test_dataset  = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UsVOzvHzZgsI"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "### Model Definition\n",
        "\n",
        "We will define a simple convolutional neural network (CNN) model for digit classification. The model architecture is as follows:\n",
        "\n",
        "1. Input layer with shape (28, 28) to accept grayscale images of size 28x28 pixels.\n",
        "2. Reshape layer to reshape the input images into a 4D tensor of shape (batch_size, 28, 28, 1).\n",
        "3. Conv2D layer with 12 filters, a kernel size of (3, 3), and ReLU activation.\n",
        "4. MaxPooling2D layer with a pool size of (2, 2) for downsampling.\n",
        "5. Flatten layer to convert the 2D feature maps into a 1D feature vector.\n",
        "6. Dense layer with 100 neurons.\n",
        "7. Dense layer with 50 neurons.\n",
        "8. Output Dense layer with 10 neurons (one for each class label) and no activation (logits).\n",
        "\n",
        "**Note:** Although the Femtosense SPU does not currently support `Conv2D` layers, TFLite files containing Conv2D models can still be generated using the `femtoflow` tool. However, be aware that attempting to compile these TFLite files with [femtocrux](https://femtocrux.femtosense.ai/en/latest/) will result in an error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO1H99cpZgsJ"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture.\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(100),\n",
        "  tf.keras.layers.Dense(50),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "46sVelmYZgsJ"
      },
      "source": [
        "### Define Training-Related Parameters\n",
        "\n",
        "Before we proceed with training the model, we need to define a few training-related parameters:\n",
        "\n",
        "1. `optimizer`: The optimization algorithm used to update the model weights. We will use the Adam optimizer.\n",
        "2. `loss_fn`: The loss function used to measure the difference between the predicted and true labels. We will use the Sparse Categorical Crossentropy loss, which is suitable for multi-class classification tasks.\n",
        "3. `metrics`: The evaluation metric used to assess the performance of the model. We will use classification accuracy.\n",
        "4. `epochs`: The number of times the training process iterates over the entire dataset. We will set it to 2 for demonstration purposes.\n",
        "5. `validation_split`: The fraction of the training dataset reserved for validation. We will use a 10% validation split.\n",
        "6. `batch_size`: The number of samples per gradient update. We will use a batch size of 512.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9TXhypmZgsK"
      },
      "outputs": [],
      "source": [
        "optimizer = 'adam'\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = ['accuracy']\n",
        "epochs = 2\n",
        "validation_split = 0.1\n",
        "batch_size = 512"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PM_M57uXZgsK"
      },
      "source": [
        "### Train the Digit Classification Model\n",
        "\n",
        "Once the model architecture and training-related parameters are defined, we can proceed to train the digit classification model. To do this, we use the `compile` method to configure the model for training by specifying the optimizer, loss function, and evaluation metric. We then use the `fit` method to start the training process with the specified batch size, number of epochs, and validation split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8PqL8eZZgsK",
        "outputId": "6e51a49f-96e4-483a-95ef-a81f6b9fe7fc"
      },
      "outputs": [],
      "source": [
        "# Train the digit classification model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  batch_size=batch_size,\n",
        "  epochs=epochs,\n",
        "  validation_split=validation_split,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-Fh7Q_qZgsL",
        "outputId": "c4794687-5cb7-43fa-a376-125d709eddf2"
      },
      "outputs": [],
      "source": [
        "_, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NBJLoab_ZgsL"
      },
      "source": [
        "## Quantize the Model Using TFLite\n",
        "\n",
        "Quantization is a technique used to reduce the memory footprint and improve the computational efficiency of a model by converting the weights and activations from floating-point representation to fixed-point representation. In this section, we will quantize the trained model using TensorFlow Lite (TFLite).\n",
        "\n",
        "### Quantize Using `TFLiteModelWrapper()` Class\n",
        "\n",
        "To perform quantization, we will use the `TFLiteModelWrapper()` class. This class provides a convenient interface for converting a TensorFlow model to a quantized TFLite model. \n",
        "\n",
        "We first define a `representative_data_gen` function that generates the representative dataset for quantization calibration. This dataset is generated using a subset of the training images and reflects the typical input data distribution.\n",
        "\n",
        "We then set the quantization mode to either `'8x16'` or `'8x8'`. The mode `'8x16'` indicates 8-bit quantization for weights and 16-bit quantization for activations, while `'8x8'` indicates 8-bit quantization for both weights and activations. We provide the model, representative dataset, and quantization mode to the `TFLiteModelWrapper()` class and specify the save path for the quantized TFLite model (`tflite_save_path`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s39mGj7aZgsL",
        "outputId": "1e8d5c19-e47a-4905-8f74-a6e3a91d4be6"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "num_samples=100\n",
        "input_name = model.input_names[0]\n",
        "output_name = model.output_names[0]\n",
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(batch_size).take(num_samples):\n",
        "    # Model has only one input so each data point has one element.\n",
        "    yield {input_name: tf.cast(input_value, dtype=tf.float32)}\n",
        "\n",
        "tflite_save_path = 'tflite_dense.tflite'\n",
        "quantize_mode = '8x16' # Or '8x8'\n",
        "model_tflite = TFLiteModelWrapper(quantize_mode=quantize_mode,\n",
        "                                  model=model,\n",
        "                                  representative_dataset=representative_data_gen,\n",
        "                                  tflite_save_path=tflite_save_path)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Yv9GpjQXZgsL"
      },
      "source": [
        "## Compare Baseline/Quantized Accuracy\n",
        "\n",
        "To evaluate the performance of the quantized model, we will compare its accuracy to the accuracy of the original (baseline) model. We define a helper function `_accuracy_mnist_` that calculates the classification accuracy for the given model on the MNIST test dataset. \n",
        "\n",
        "We then use this function to calculate the accuracy of both the quantized TFLite model (`model_tflite`) and the original TensorFlow model (`model`). The results will help us understand the impact of quantization on the model's performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IWCeXc3ZgsM",
        "outputId": "455b1a33-e4c9-482d-a2af-1649558fee81"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _accuracy_mnist_(model, test_dataset, output_name='output_0', input_name=input_name):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  for batch_id, (x_batch, y_batch) in enumerate(test_dataset):\n",
        "    y_pred = model({input_name: x_batch}) #[output_name]\n",
        "    if isinstance(y_pred, dict):\n",
        "      y_pred = y_pred[output_name]\n",
        "    num_samples += len(y_batch)\n",
        "    num_correct += sum(np.argmax(y_pred, axis=1)== y_batch.numpy())\n",
        "\n",
        "  return num_correct/num_samples\n",
        "\n",
        "\n",
        "acc = _accuracy_mnist_(model_tflite, test_dataset)\n",
        "print(\"TFLite Quantized Accuracy\", acc)\n",
        "\n",
        "acc_orig = _accuracy_mnist_(model, test_dataset)\n",
        "print(\"Baseline Model Accuracy\", acc_orig)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ad41ba704b490a86eec1bcebab6abb4e034d27c24fdf165a52d45ebef6dd8584"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
