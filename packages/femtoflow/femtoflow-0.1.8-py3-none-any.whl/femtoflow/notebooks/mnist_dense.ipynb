{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TurLlDQnVOUb"
      },
      "source": [
        "# Pruning and Quantization on MNIST Digit Recognition\n",
        "\n",
        "In this notebook, we demonstrate the application of pruning and quantization techniques to optimize a neural network model for the MNIST Digit Recognition task. Our objective is to create an efficient model with a reduced memory footprint while maintaining high accuracy.\n",
        "\n",
        "## Overview\n",
        "- **Task:** MNIST Digit Recognition\n",
        "- **Model:** Neural Network with Dense layers\n",
        "- **Layers to Prune:** Dense layers\n",
        "- **Pencil Size:** 4 (for pruning)\n",
        "\n",
        "## Tutorial Flow\n",
        "\n",
        "1. **Pruning Using `model.fit(_)` method:** Prune a Tensorflow model using `model.fit()` method. The goal is to sparsify the model while maintaining its performance.\n",
        "\n",
        "4. **TFLite Conversion:** The sparsified-model, equipped with the sparse weights, is then converted to TFLite format for deployment on edge devices.\n",
        "\n",
        "5. Compiling TFLite model using [femtocrux](https://femtocrux.femtosense.ai/en/latest/)\n",
        "\n",
        "6. Generating Program Files from the Compiled Model using [femtodriverpub](https://github.com/femtosense/femtodriverpub) , and loading them onto SPU. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6WsHSGCcVOUe"
      },
      "source": [
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f01_zXAwVOUe",
        "outputId": "1cb61f99-3275-46c5-ce73-bc34acdc1882",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# ! pip install femtoflow --quiet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AXvK_ldbVaaU"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOtflp1lVOUe"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "from femtoflow.quantization.quantize_tflite import TFLiteModelWrapper\n",
        "from femtoflow.sparsity.prune import PruneHelper\n",
        "from femtoflow.utils.plot import plot_prune_mask\n",
        "from femtoflow.utils.metrics import calculate_sparsity, get_gzipped_model_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6meIk1kMVOUf"
      },
      "source": [
        "## Load MNIST Dataset\n",
        "\n",
        "In this section, we load the MNIST dataset, which is a popular dataset for handwritten digit recognition. It consists of grayscale images of handwritten digits, each of size 28x28 pixels, along with corresponding labels indicating the digit (0-9) represented by each image.\n",
        "\n",
        "### Data Preparation\n",
        "\n",
        "1. **Load MNIST Data:** We use the Keras built-in `mnist` module to load the dataset, which is split into training and testing sets.\n",
        "\n",
        "2. **Normalize Data:** We normalize the input images by scaling the pixel values to the range [0, 1]. This is achieved by dividing each pixel value by 255 (the maximum pixel value in grayscale images).\n",
        "\n",
        "3. **Create TensorFlow Datasets:** We use the `tf.data.Dataset.from_tensor_slices` method to create TensorFlow datasets for both the training and testing sets. We configure the datasets to provide data in batches using the `batch` method. The batch size is set to 1024, and `drop_remainder=True` ensures that any incomplete batches are discarded.\n",
        "\n",
        "The resulting `train_dataset` and `test_dataset` are ready for use in training and evaluating the model, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNtR4fTcVOUf",
        "outputId": "eae19c14-f74a-4196-b4af-02bee491d55b"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "num_samples_train, hx, hy = train_images.shape\n",
        "train_images = tf.reshape(train_images, (num_samples_train, hx * hy))\n",
        "\n",
        "num_samples_test, hx, hy = test_images.shape\n",
        "test_images = tf.reshape(test_images, (num_samples_test, hx * hy))\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(BATCH_SIZE, drop_remainder=True)\n",
        "test_dataset  = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "871Ou-_jVOUg"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "In this section, we define the architecture of the neural network model that we will train and optimize for the MNIST digit recognition task. The model consists of convolutional, pooling, and dense layers.\n",
        "\n",
        "### Define Model Architecture\n",
        "\n",
        "1. **Input Layer:** The input layer is configured to accept Flattened Grayscale images of size 784x1 pixels.\n",
        "\n",
        "6. **Dense Layers:** A sequence of dense (fully connected) layers with different numbers of units. The final dense layer has 10 units corresponding to the 10 possible digits (0-9).\n",
        "\n",
        "The defined model architecture is ready for training on the MNIST dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGlAu2p5VOUg"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture.\n",
        "# Define the model architecture.\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(784, )),\n",
        "  tf.keras.layers.Dense(200),\n",
        "  tf.keras.layers.Dense(100),\n",
        "  tf.keras.layers.Dense(50),\n",
        "  tf.keras.layers.Dense(10),\n",
        "])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iu3jZKT5VOUg"
      },
      "source": [
        "### Define Training-Related Parameters\n",
        "\n",
        "Before training the model, we need to define various parameters and configurations that will be used during the training process:\n",
        "\n",
        "1. **Optimizer:** We choose the `'adam'` optimizer, which is an adaptive optimization algorithm commonly used for training neural networks.\n",
        "\n",
        "2. **Loss Function:** We use the `SparseCategoricalCrossentropy` loss function with `from_logits=True`. This loss function is suitable for multi-class classification tasks, such as digit recognition. The `from_logits=True` option indicates that the model's output is not yet passed through the softmax activation function.\n",
        "\n",
        "3. **Metrics:** We monitor the `'accuracy'` metric during training to evaluate the performance of the model.\n",
        "\n",
        "4. **Epochs:** The number of epochs is set to `2`. An epoch is a complete pass through the entire training dataset.\n",
        "\n",
        "5. **Validation Split:** We specify a validation split of `0.1`, which means that 10% of the training data will be used as a validation set to evaluate the model's performance during training.\n",
        "\n",
        "6. **Batch Size:** We set the batch size to `512`, which determines the number of samples used in each update of the model weights.\n",
        "\n",
        "With these parameters defined, we can proceed with training the model on the MNIST dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp69qahEVOUh"
      },
      "outputs": [],
      "source": [
        "optimizer = 'adam'\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = ['accuracy']\n",
        "epochs = 2\n",
        "validation_split = 0.1\n",
        "batch_size = 512\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CfjqezrkVOUh"
      },
      "source": [
        "### Train the Digit Classification Model\n",
        "\n",
        "With the model architecture defined and the training-related parameters configured, we can proceed with training the digit classification model on the MNIST dataset.\n",
        "\n",
        "1. **Compile the Model:** We use the `compile` method to configure the model for training. We pass the optimizer, loss function, and evaluation metrics as arguments.\n",
        "\n",
        "2. **Fit the Model:** We use the `fit` method to train the model on the training data. We provide the training images and corresponding labels, the batch size, the number of epochs, and the validation split. The `fit` method will perform forward and backward propagation, update the model weights, and monitor the training and validation accuracy over the specified number of epochs.\n",
        "\n",
        "During training, the model's performance is evaluated on both the training and validation sets at the end of each epoch. The training accuracy indicates how well the model is fitting the training data, while the validation accuracy indicates how well the model generalizes to unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OObASg0-VOUh",
        "outputId": "a4e09b81-5aea-48a7-8739-08cd2eb423a6"
      },
      "outputs": [],
      "source": [
        "# Train the digit classification model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  batch_size=batch_size,\n",
        "  epochs=epochs,\n",
        "  validation_split=validation_split,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hldd9845VOUi"
      },
      "source": [
        "### Get Baseline Model Accuracy\n",
        "\n",
        "After training the digit classification model, it is important to evaluate its performance on the test dataset. The test dataset consists of images and labels that were not used during training, so it provides an unbiased evaluation of the model's ability to generalize to new data.\n",
        "\n",
        "We use the `evaluate` method to calculate the test accuracy of the model. The method takes the test images and corresponding labels as input, performs forward propagation through the model, and compares the predicted labels with the true labels to calculate the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnME0Z_tVOUi",
        "outputId": "7804f717-9ced-4c8c-d937-98f143285f4b"
      },
      "outputs": [],
      "source": [
        "_, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Md2eSLUdVOUi"
      },
      "source": [
        "## Prune the Model\n",
        "\n",
        "Pruning is a model optimization technique that reduces the number of parameters in the model by setting some of them to zero. This results in a sparse model that requires less memory and computational resources. In this section, we will apply structured pruning to the digit classification model to obtain a sparser version of the model.\n",
        "\n",
        "### Define the `prune_helper = PruneHelper()` Class\n",
        "\n",
        "To facilitate the pruning process, we define an instance of the `PruneHelper` class. This class provides a set of utility functions for applying structured pruning to specific layers of the model.\n",
        "\n",
        "1. **Pencil Size:** The pencil size specifies the granularity of the pruning mask applied to the model weights. Supported Pencil Sizes are `8` and `4`.\n",
        "\n",
        "2. **Pencil Pooling Type:** The pencil pooling type determines the pooling operation used in the pruning mask calculation. The options are 'AVG' (average pooling) and 'MAX' (max pooling).\n",
        "\n",
        "3. **Prune Scheduler:** The prune scheduler controls the sparsity schedule over the training epochs. The options are 'linear' (linear increase in sparsity), 'constant' (constant sparsity), and 'poly_decay' (polynomial decay in sparsity).\n",
        "\n",
        "We configure these parameters and create an instance of the `PruneHelper` class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHJGluJMVOUj"
      },
      "outputs": [],
      "source": [
        "pencil_size = 4\n",
        "pencil_pooling_type = 'AVG'\n",
        "prune_scheduler = 'linear'  # 'constant' # 'poly_decay'\n",
        "prune_helper = PruneHelper(pencil_size=pencil_size,\n",
        "                             pencil_pooling_type=pencil_pooling_type,\n",
        "                             prune_scheduler=prune_scheduler)\n",
        "                             "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NnSsWXA0VOUj"
      },
      "source": [
        "### Apply Pruning Wrappers to the Model\n",
        "\n",
        "The `PruneHelper` class applies pruning wrappers to the layers of the model that we wish to sparsify. The output is a new model, `model_to_prune`, with pruning wrappers applied to the specified layers.\n",
        "\n",
        "To configure the pruning process, we define the following additional parameters:\n",
        "\n",
        "1. **Layers to Prune:** A list specifying the types of layers we want to prune. In this case, we choose to prune only the Dense layers.\n",
        "\n",
        "2. **Initial Sparsity:** The initial sparsity level of the model at the beginning of the pruning process.\n",
        "\n",
        "3. **Final Sparsity:** The target sparsity level of the model at the end of the pruning process.\n",
        "\n",
        "4. **Begin Step:** The training step at which pruning begins.\n",
        "\n",
        "5. **End Step:** The training step at which pruning ends. This is calculated based on the total number of training steps per epoch and the total number of epochs.\n",
        "\n",
        "6. **Prune Frequency:** The frequency (in number of training steps) at which the pruning mask is updated.\n",
        "\n",
        "7. **Power:** The exponent used in the polynomial decay schedule for sparsity (only applicable if the prune scheduler is set to 'poly_decay').\n",
        "\n",
        "We pass these parameters to the `PruneHelper` instance to obtain the model with pruning wrappers applied.\n",
        "\n",
        "The `model_to_prune` now has pruning wrappers applied to the Dense layers, and it is ready for training with sparsity. The goal is to achieve a high level of sparsity while preserving the model's accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6bKrBZcVOUj",
        "outputId": "c13a099f-f34a-472c-f0dd-945eda63482a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Define additional parameters for pruning\n",
        "\"\"\"\n",
        "layers_to_prune = [tf.keras.layers.Dense] # Layers we want to prune \n",
        "initial_sparsity = 0.2\n",
        "final_sparsity = 0.6\n",
        "begin_step = 0\n",
        "end_step = len(train_dataset)*epochs # Let function implicitly find end_step\n",
        "prune_frequency = 100\n",
        "power = 3\n",
        "\n",
        "model_to_prune = prune_helper(model=model,\n",
        "                                layers_to_prune=layers_to_prune,\n",
        "                                initial_sparsity=initial_sparsity,\n",
        "                                final_sparsity=final_sparsity,\n",
        "                                begin_step=begin_step,\n",
        "                                end_step=end_step,\n",
        "                                prune_frequency=prune_frequency,\n",
        "                                power=power)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGz3AAxSVOUk",
        "outputId": "c644055c-3e7e-4577-ab7f-e79a1092a16a"
      },
      "outputs": [],
      "source": [
        "model_to_prune.layers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cq4Sp9IVOUk"
      },
      "source": [
        "### Train the Model with Sparsity\n",
        "\n",
        "After applying the pruning wrappers to the model, we proceed with training the model to introduce sparsity into the weights. During training, the sparsity level of the weights is gradually increased according to the pruning schedule defined earlier.\n",
        "\n",
        "To achieve this, we use the `tfmot.sparsity.keras.UpdatePruningStep()` callback as part of the training process. This callback is responsible for updating the pruning mask at regular intervals, resulting in a gradual increase in sparsity.\n",
        "\n",
        "We compile the model and use the `.fit()` function to start training with sparsity. The `UpdatePruningStep()` callback is included in the list of callbacks for the `.fit()` function.\n",
        "\n",
        "During training, the pruning mask is updated based on the prune_frequency parameter, and the sparsity level of the weights is progressively increased. At the end of training, we obtain a pruned model with sparse weights while aiming to maintain a similar level of accuracy as the original model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3-1I42bVOUk",
        "outputId": "78d02d86-40f0-4556-df6d-6289ae85157f"
      },
      "outputs": [],
      "source": [
        "model_to_prune.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "model_to_prune.fit(train_dataset,\n",
        "                   epochs=epochs, \n",
        "                   callbacks=[tfmot.sparsity.keras.UpdatePruningStep()])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xFZeHfSQVOUk"
      },
      "source": [
        "### Evaluate the Pruned Model\n",
        "\n",
        "After training the model with sparsity, it is important to evaluate the accuracy of the pruned model to ensure that the pruning process has not adversely impacted the model's performance. To do this, we use the `.evaluate()` function on the pruned model and test it against the test dataset.\n",
        "\n",
        "The `prune_accuracy` represents the accuracy of the pruned model on the test dataset. Ideally, the pruned model should have an accuracy close to that of the original baseline model while benefiting from the reduced model size and sparsity.\n",
        "\n",
        "Comparing the accuracy of the pruned model (`prune_accuracy`) with the baseline model accuracy (`baseline_model_accuracy`) allows us to assess the impact of pruning on the model's performance. A successful pruning process should achieve a good balance between accuracy and sparsity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DH0Jb8biVOUl",
        "outputId": "d417197a-e20a-48c6-c224-d11c951acb2c"
      },
      "outputs": [],
      "source": [
        "_, prune_accuracy = model_to_prune.evaluate(\n",
        "   test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Pruned test accuracy:', prune_accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cd7YDv3jVOUl"
      },
      "source": [
        "### Strip Pruning Wrapper to Obtain the Final Pruned Model\n",
        "\n",
        "The pruned model (`model_to_prune`) contains pruning wrappers applied to the layers we want to sparsify. These wrappers are used to introduce and manage sparsity during the training process. After pruning is complete, we can remove these wrappers to obtain the final pruned model with the sparse weights (i.e., the final pruning mask applied to the weights).\n",
        "\n",
        "To remove the pruning wrappers, we use the `tfmot.sparsity.keras.strip_pruning()` function. This function takes the pruned model as input and returns the pruned model with the sparse weights, but without the pruning wrappers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LADEnHJEVOUl"
      },
      "outputs": [],
      "source": [
        "model_pruned_stripped = tfmot.sparsity.keras.strip_pruning(model_to_prune)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mjMOw0HTVOUl"
      },
      "source": [
        "### Visualize Pruned Weights\n",
        "\n",
        "After pruning, it is helpful to visualize the pruned weights to observe the sparsity pattern achieved by the pruning process. We can plot the pruned weights as a binary mask, where white pixels represent non-zero (preserved) weights and black pixels represent zero (pruned) weights.\n",
        "\n",
        "To do this, we use the `plot_prune_mask` function, which takes the pruned weight matrix and some visualization parameters as inputs. We iterate over each pruned layer in the model, extract its weight matrix, and plot the binary mask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z9UXUM0VOUm",
        "outputId": "9f0be01c-d976-46e0-c4cf-13f0ee26ab42"
      },
      "outputs": [],
      "source": [
        "trainable_weights_dict = {weight.name: weight.numpy() for weight in model_pruned_stripped.trainable_weights}\n",
        "trainable_weights_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eyfGeTV5VOUm",
        "outputId": "2cf4de43-1e73-4489-d531-408c26b0d99b"
      },
      "outputs": [],
      "source": [
        "\n",
        "base_path = 'mnist_dense_pencil_4'\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "max_len_axis = 24\n",
        "for layer_name, layer in trainable_weights_dict.items():\n",
        "    title = f\"{layer_name}-shape-opxip-{layer.T.shape}\"\n",
        "    save_path = f\"{base_path}/{layer_name.replace('/', '-')}-prune-mask.png\"\n",
        "    plot_prune_mask(data=layer, axis_stride=pencil_size, max_xlen=max_len_axis, max_ylen=max_len_axis, title=title, save_path=save_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz6e5-etVOUn"
      },
      "source": [
        "### Calculate Sparse Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqFsHCZPVOUn",
        "outputId": "18faf36f-cd6b-4aa4-ce3d-1eec61a5c121"
      },
      "outputs": [],
      "source": [
        "sparsity_dict = {}\n",
        "for layer_name, layer_weight in trainable_weights_dict.items():\n",
        "    sparsity_dict[layer_name] = calculate_sparsity(layer_weight)\n",
        "print('sparsity_dict', sparsity_dict)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L7QqYKWqVOUo"
      },
      "source": [
        "## Quantize the Pruned Model using TFLite\n",
        "\n",
        "After pruning, the next step is to quantize the pruned model to further reduce its memory footprint and improve its computational efficiency. Quantization involves converting the weights and activations from floating-point representation to fixed-point representation. In this section, we will quantize the pruned model using TensorFlow Lite (TFLite).\n",
        "\n",
        "### Quantize using `TFLiteModelWrapper()` class\n",
        "\n",
        "To perform quantization, we will use the `TFLiteModelWrapper()` class. This class provides a convenient interface for converting a TensorFlow model to a quantized TFLite model. We first define a `representative_data_gen` function that generates representative data for quantization calibration. This data should reflect the typical input distribution that the model will encounter during inference.\n",
        "\n",
        "We can choose either `'8x8'` quantization mode (Int8 weights and Int8 activations) or `'8x16'` quantization mode (Int8 weights and Int16 activations). We provide the pruned model, representative dataset, and quantization mode to the `TFLiteModelWrapper()` class and specify the save path for the quantized TFLite model (`tflite_save_path`).\n",
        "\n",
        "The `model_tflite` is the quantized TFLite model that is optimized for deployment on resource-constrained devices. It benefits from both the sparsity achieved during pruning and the reduced memory footprint achieved through quantization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZS_uUrAVOUp",
        "outputId": "5be4d7f8-5114-4881-bbe9-56b19cfdada8"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "num_samples=100\n",
        "input_name = model_pruned_stripped.input_names[0]\n",
        "output_name = model_pruned_stripped.output_names[0]\n",
        "def representative_data_gen():\n",
        "  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(batch_size).take(num_samples):\n",
        "    # Model has only one input so each data point has one element.\n",
        "    yield {input_name: tf.cast(input_value, dtype=tf.float32)}\n",
        "\n",
        "tflite_save_path = 'tflite_dense.tflite'\n",
        "quantize_mode = '8x16' # or '8x8'\n",
        "model_tflite = TFLiteModelWrapper(quantize_mode=quantize_mode,\n",
        "                                  model=model_pruned_stripped,\n",
        "                                  representative_dataset=representative_data_gen,\n",
        "                                  tflite_save_path=tflite_save_path)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kf8lx3rTVOUp"
      },
      "source": [
        "### Check Performance of TFLite Model\n",
        "\n",
        "After quantizing the pruned model, we want to evaluate its performance on the test dataset to ensure that the quantization process did not adversely affect the model's accuracy. We will define a helper function `_accuracy_mnist_` that calculates the classification accuracy for a given model and dataset.\n",
        "\n",
        "The `_accuracy_mnist_` function takes the model, test dataset, and optional names of the output and input tensors as arguments. It performs inference on each batch of the test dataset and compares the model's predictions with the ground truth labels to calculate the overall accuracy.\n",
        "\n",
        "We will then calculate the accuracy for the TFLite pruned and quantized model (`model_tflite`), the original baseline model (`model`), and the pruned model without quantization (`model_pruned_stripped`). Comparing these accuracies will provide insight into the effectiveness of the pruning and quantization processes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9sRrZZcVOUp",
        "outputId": "ee1bd4f5-7edf-47c0-e203-30303153d6a6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def _accuracy_mnist_(model, test_dataset, output_name='output_0', input_name=input_name):\n",
        "  num_correct = 0\n",
        "  num_samples = 0\n",
        "  for batch_id, (x_batch, y_batch) in enumerate(test_dataset):\n",
        "    y_pred = model({input_name: x_batch}) #[output_name]\n",
        "    if isinstance(y_pred, dict):\n",
        "      y_pred = y_pred[output_name]\n",
        "    num_samples += len(y_batch)\n",
        "    num_correct += sum(np.argmax(y_pred, axis=1)== y_batch.numpy())\n",
        "\n",
        "  return num_correct/num_samples\n",
        "\n",
        "\n",
        "acc = _accuracy_mnist_(model_tflite, test_dataset)\n",
        "print(\"TFLite Pruned+Quantized Accuracy\", acc)\n",
        "\n",
        "acc_orig = _accuracy_mnist_(model, test_dataset)\n",
        "print(\"Baseline Model Accuracy\", acc_orig)\n",
        "\n",
        "acc_model_pruned_stripped = _accuracy_mnist_(model_pruned_stripped, test_dataset)\n",
        "print(\"Pruend Model Accuracy\", acc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rXeyrCsUVOUq"
      },
      "source": [
        "### Check Model Sizes\n",
        "\n",
        "After completing the pruning and quantization processes, it is important to check the model sizes to confirm that the optimizations have effectively reduced the memory footprint of the models. We will compare the sizes of three models: the original baseline model, the pruned model without quantization, and the TFLite pruned and quantized model.\n",
        "\n",
        "To calculate the sizes, we will save each model to a temporary file and then use gzip compression to measure the compressed size. This will give us an estimate of the storage space required for each model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sJvg703VOUq",
        "outputId": "8b6f8018-c6cd-43c2-895e-96764acd9076"
      },
      "outputs": [],
      "source": [
        "_, keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model, keras_file, include_optimizer=False)\n",
        "print('Saved baseline model to:', keras_file)\n",
        "\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model_pruned_stripped, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)\n",
        "\n",
        "pruned_tflite_file = tflite_save_path\n",
        "print(\"TFLite File was generated at:\", pruned_tflite_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQM4OjGJVOUq",
        "outputId": "fd47c717-2830-45a3-b2a6-26a52070167a"
      },
      "outputs": [],
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compiling TFLite model using [femtocrux](https://femtocrux.femtosense.ai/en/latest/) and generating Memory Image BitFile\n",
        "\n",
        "Next, we will compile the generated TFLite model with Femtocrux. Compiling the model using Femtocrux is a necessary step in deploying the Tensorflow model on Femtosense's SPU.\n",
        "\n",
        "We will need to have Docker installed and instantiate a CompilerClient. This will allow us to make API calls to the Femtosense's compiler. We can call the `compile` method of femtocrux to produce the `Memory Image bitfile`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from femtocrux import CompilerClient, TFLiteModel\n",
        "client = CompilerClient()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flatbuffer = model_tflite.instance.flatbuffer\n",
        "signature_name = model_tflite.instance.signature_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bitstream = client.compile(    \n",
        "    TFLiteModel(flatbuffer=flatbuffer, signature_name=signature_name)\n",
        ")\n",
        "# Write to a file for later use\n",
        "with open('my_bitfile.zip', 'wb') as f: \n",
        "    f.write(bitstream)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Program Files, and loading onto SPU. \n",
        "The memory image bitfile zip can then be converted to Program Files using [femtodriverpub](https://github.com/femtosense/femtodriverpub).\n",
        "\n",
        "Once generated, these Program Files can be transferred to an SD card, which can then be inserted into Femtosense's SPU. \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Install `Femtodriverpub`\n",
        "##### Step One - Clone femtodriverpub from Github and Install it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! git clone https://github.com/femtosense/femtodriverpub.git; cd femtodriverpub; pip install -e ."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Unzip the `my_bitfile.zip` zip folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! rm -rf 'my_bitfile'\n",
        "! unzip 'my_bitfile.zip' -d 'my_bitfile'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generate the Program Files to load onto SPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! python femtodriverpub/femtodriverpub/run/sd_from_femtocrux.py 'my_bitfile'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The Program Files should be generated at `apb_records` folder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! ls 'apb_records'"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The contents inside `apb_records` can be loaded onto a SD card, which can then be inserted onto the SPU! Congratulations, your model is now ready to deploy on the SPU!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ad41ba704b490a86eec1bcebab6abb4e034d27c24fdf165a52d45ebef6dd8584"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
