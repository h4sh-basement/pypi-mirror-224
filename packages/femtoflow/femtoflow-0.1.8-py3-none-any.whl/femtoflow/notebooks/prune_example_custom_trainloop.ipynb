{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yGO9IWdIWtEv"
      },
      "source": [
        "# Simple Pruning Example (with Custom Training Loop)\n",
        "\n",
        "In this section, we will demonstrate a simple example of pruning a neural network using a custom training loop. The example will cover how to apply structured pruning to a neural network model using the `PruneHelper` class from `femtoflow`. We will then train the pruned model using a custom training loop and evaluate its performance. This example aims to provide an understanding of how to implement structured pruning in a custom training pipeline.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wvBmux_qWtEv"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjayVYk7WtEw",
        "outputId": "2ea81089-3c7a-47a8-be10-54514115d6d8",
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# ! pip install femtoflow --quiet"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j1nRgoOYWtEw"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy719WctWtEx"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "\n",
        "from femtoflow.sparsity.prune import PruneHelper\n",
        "from femtoflow.utils.plot import plot_prune_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VoolC0TuWtEx"
      },
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GJXc2glmWtEy"
      },
      "source": [
        "## MNIST Dataset Download\n",
        "\n",
        "In this example, we will use the MNIST dataset, which is a widely used dataset for handwritten digit recognition. The dataset consists of 60,000 training images and 10,000 test images, each of which is a grayscale image with a size of 28x28 pixels. Each image is labeled with the corresponding digit (0-9) that it represents.\n",
        "\n",
        "We will download the MNIST dataset, normalize the images so that each pixel value is between 0 and 1, and then prepare the dataset for training and evaluation. We will create TensorFlow Datasets (`tf.data.Dataset`) for the training and test sets, and batch the data using a batch size of 1024.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5MtnR-NWtEy",
        "outputId": "b3515973-e69e-4893-a423-98e7c53e22cd"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "mnist = keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the input image so that each pixel value is between 0 and 1.\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "BATCH_SIZE = 1024\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(BATCH_SIZE, drop_remainder=True)\n",
        "test_dataset  = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oh6ObiheWtEz"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "In this section, we will define and train a convolutional neural network (CNN) for digit classification using the MNIST dataset. The model architecture consists of an input layer, a reshape layer, a Conv2D layer, a MaxPooling2D layer, a Flatten layer, and three Dense layers.\n",
        "\n",
        "### Model Definition\n",
        "\n",
        "We will use Keras Sequential API to define our model architecture. The input layer accepts grayscale images of size 28x28 pixels. The Conv2D layer applies 12 filters with a kernel size of 3x3 and ReLU activation function. The MaxPooling2D layer reduces the spatial dimensions by taking the maximum value from each 2x2 window. The Flatten layer flattens the 3D output into a 1D array. The final three Dense layers contain 100, 50, and 10 units, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7qow9SxWtE0"
      },
      "outputs": [],
      "source": [
        "# Define the model architecture.\n",
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(100),\n",
        "  tf.keras.layers.Dense(50),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-_-KdgqQWtE0"
      },
      "source": [
        "### Define Training Related Params\n",
        "\n",
        "Before we start training the model, we need to define some training-related parameters such as the optimizer, loss function, evaluation metrics, number of epochs, validation split, and batch size.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yLnXjPJWtE0"
      },
      "outputs": [],
      "source": [
        "optimizer = 'adam'\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metrics = ['accuracy']\n",
        "epochs = 2\n",
        "validation_split = 0.1\n",
        "batch_size = 512"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yvg04-83WtE0"
      },
      "source": [
        "### Train the Digit Classification Model\n",
        "\n",
        "After defining the model architecture and training-related parameters, we are ready to train the digit classification model. We will compile the model using the optimizer, loss function, and evaluation metrics defined earlier. Then, we will start the training process using the `fit` method.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y56phhEjWtE1",
        "outputId": "1eb14805-54df-4428-89de-682b333842f1"
      },
      "outputs": [],
      "source": [
        "# Train the digit classification model\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "model.fit(\n",
        "  train_images,\n",
        "  train_labels,\n",
        "  batch_size=batch_size,\n",
        "  epochs=epochs,\n",
        "  validation_split=validation_split,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIUzW-SOWtE1",
        "outputId": "7f08ea78-0837-4ced-b6e6-937255700363"
      },
      "outputs": [],
      "source": [
        "_, baseline_model_accuracy = model.evaluate(test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', baseline_model_accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "piQJxg_8WtE1"
      },
      "source": [
        "## Prune the Model\n",
        "\n",
        "After training the model, we will perform structured pruning to sparsify the model's weights. For this purpose, we will use the `PruneHelper` class, which provides utilities for applying structured pruning to the model.\n",
        "\n",
        "### Define the `prune_helper = PruneHelper()` Class\n",
        "\n",
        "We first define the `PruneHelper` class by specifying various parameters related to the pruning process, such as `pencil_size`, `pencil_pooling_type`, and `prune_scheduler`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgv7n43YWtE1"
      },
      "outputs": [],
      "source": [
        "pencil_size = 4\n",
        "pencil_pooling_type = 'AVG'\n",
        "prune_scheduler = 'linear'  # 'constant' # 'poly_decay'\n",
        "prune_helper = PruneHelper(pencil_size=pencil_size,\n",
        "                           pencil_pooling_type=pencil_pooling_type,\n",
        "                           prune_scheduler=prune_scheduler,\n",
        "                           min_parameter_thresh=0)\n",
        "                             "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6dyekGozWtE2"
      },
      "source": [
        "### Apply Pruning to the Model\n",
        "\n",
        "To apply structured pruning to the model, we will call the `PruneHelper` instance with additional parameters related to the pruning process. The `PruneHelper` class will apply pruning masks to the specified layers of the model that we want to sparsify.\n",
        "\n",
        "#### Define additional parameters for pruning\n",
        "- `layers_to_prune`: A list of Keras layer classes that we want to prune. \n",
        "  In this example, we want to prune only the dense layers, so we specify `[tf.keras.layers.Dense]`.\n",
        "- `initial_sparsity`: The initial sparsity level (fraction of weights to be set to zero) \n",
        "  at the start of the pruning process. We set it to `0.2`.\n",
        "- `final_sparsity`: The final sparsity level (fraction of weights to be set to zero) \n",
        "  at the end of the pruning process. We set it to `0.6`.\n",
        "- `begin_step`: The step at which to start pruning. We set it to `0`.\n",
        "- `end_step`: The step at which to end pruning. We calculate it as the total number \n",
        "  of training steps, which is `len(train_dataset) * epochs`.\n",
        "- `prune_frequency`: The frequency (in number of steps) at which to update the pruning mask. \n",
        "  We set it to `100`.\n",
        "- `power`: The exponent for polynomial decay of the sparsity level. This parameter is used \n",
        "  when the `prune_scheduler` is set to `'poly_decay'`. We set it to `3`.\n",
        "- `force_skip_layers`: A list of layer names to exclude from pruning. In this example, we want \n",
        "  to exclude a specific dense layer with the name `'dense_1/kernel:0'` from the pruning process, \n",
        "  so we specify `['dense_1/kernel:0']`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cYosQUmWtE2",
        "outputId": "c528e95d-5338-4946-cd94-a3ff3044c0e6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Define additional parameters for pruning\n",
        "\"\"\"\n",
        "layers_to_prune = [tf.keras.layers.Dense] # Layers we want to prune \n",
        "initial_sparsity = 0.2\n",
        "final_sparsity = 0.6\n",
        "begin_step = 0\n",
        "end_step = len(train_dataset)*epochs # Let function implicitly find end_step\n",
        "prune_frequency = 100\n",
        "power = 3\n",
        "\n",
        "model_to_prune = prune_helper(model=model,\n",
        "                            layers_to_prune=layers_to_prune,\n",
        "                            initial_sparsity=initial_sparsity,\n",
        "                            final_sparsity=final_sparsity,\n",
        "                            begin_step=begin_step,\n",
        "                            end_step=end_step,\n",
        "                            prune_frequency=prune_frequency,\n",
        "                            power=power,\n",
        "                            force_skip_layers=['dense_1/kernel:0'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rAk9wYFwWtE2"
      },
      "source": [
        "#### Note how Pruning Wrappers are applied to layers to be pruned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmCrJ2GuWtE2",
        "outputId": "eaa09e77-9513-4662-d76e-dcedd55f74a7"
      },
      "outputs": [],
      "source": [
        "model_to_prune.layers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "T8bQljjTWtE3"
      },
      "source": [
        "### Perform Training-With-Sparsity on `model_to_prune` with `tfmot.sparsity.keras.UpdatePruningStep()` applied in a Custom Training Loop\n",
        "\n",
        "In this section, we will define a custom training loop and perform training while inducing sparsity to the `model_to_prune`. To achieve this, we will use instances of `tfmot.sparsity.keras.UpdatePruningStep()` and call specific methods (`set_model`, `on_train_begin`, `on_train_batch_begin`, `on_epoch_end`) at specific points (Marked **1-4** in `custom_sparsity_train_loop` below) in the training loop, to successfully induce sparsity.\n",
        "\n",
        "Reference: https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEjldFQaWtE3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_sparsity_train_loop(model_to_prune, \n",
        "                                optimizer, \n",
        "                                metrics, \n",
        "                                loss_fn, \n",
        "                                train_dataset, \n",
        "                                val_dataset=None,\n",
        "                                num_epochs=2):\n",
        "    \"\"\"\n",
        "    Custom Training Loop, with instances of tfmot.sparsity.keras.UpdatePruningStep() \n",
        "    called in specific points of the training loop (marked 1-4) to induce sparsity.\n",
        "    Reference: https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide.md\n",
        "    \"\"\"\n",
        "    model_to_prune.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)\n",
        "\n",
        "    \"\"\"\n",
        "    1) Define prune_step and attach Model to prune_step callback\n",
        "    \"\"\"\n",
        "    prune_step = tfmot.sparsity.keras.UpdatePruningStep()\n",
        "    prune_step.set_model(model_to_prune)\n",
        "\n",
        "    \"\"\"\n",
        "    2) prune_step.on_train_begin() call\n",
        "    \"\"\"\n",
        "    prune_step.on_train_begin() # call bac\n",
        "    for epoch in range(0, num_epochs):\n",
        "        print(f\"Processing epoch {epoch}\")\n",
        "        for batch_id, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "            \n",
        "            \"\"\"\n",
        "            3) prune_step.on_train_batch_begin() call\n",
        "            \"\"\"\n",
        "            prune_step.on_train_batch_begin(batch=-1) # run pruning callback\n",
        "\n",
        "            model_to_prune.train_on_batch(x_batch, y_batch)\n",
        "\n",
        "        \"\"\"\n",
        "        4) prune_step.on_epoch_end() call\n",
        "        \"\"\"\n",
        "        prune_step.on_epoch_end(batch=-1) # run pruning callback\n",
        "\n",
        "    if val_dataset:\n",
        "        _, val_acc = model_to_prune.evaluate(val_dataset, verbose=0)\n",
        "        print(f\"Validation Accuracy Epoch {epoch}: {val_acc}\")\n",
        "\n",
        "    return model_to_prune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GowJV4amWtE3",
        "outputId": "382d788f-71ac-47ee-d903-7a9e19bbabdd"
      },
      "outputs": [],
      "source": [
        "model_to_prune = custom_sparsity_train_loop(model_to_prune=model_to_prune,\n",
        "                                            optimizer=optimizer,\n",
        "                                            metrics=metrics, \n",
        "                                            loss_fn=loss_fn, \n",
        "                                            train_dataset=train_dataset, \n",
        "                                            val_dataset=test_dataset,\n",
        "                                            num_epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9fFONBVWtE3",
        "outputId": "c7fcd2f1-22aa-4c91-ae03-f61c03f79232"
      },
      "outputs": [],
      "source": [
        "_, prune_accuracy = model_to_prune.evaluate(\n",
        "   test_images, test_labels, verbose=0)\n",
        "\n",
        "print('Pruned test accuracy:', prune_accuracy)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5MUfUCElWtE4"
      },
      "source": [
        "### Apply `tfmot.sparsity.keras.strip_pruning()` to Remove Sparse Layer Wrappers and Get the Model with Sparse Weights (Prune Masks Applied)\n",
        "\n",
        "After pruning, the pruned model (`model_to_prune`) contains a `tfmot.sparsity.keras.prune_low_magnitude()` wrapper around the layers that were pruned. To obtain the final pruned model with sparse weights, we need to remove this wrapper.\n",
        "\n",
        "The `strip_pruning()` function is used for this purpose. It removes the pruning wrapper and returns the pruned model with the final pruning mask applied to the weights, resulting in the desired sparse weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDEDPCQXWtE4"
      },
      "outputs": [],
      "source": [
        "model_pruned_stripped = tfmot.sparsity.keras.strip_pruning(model_to_prune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6ZgI4qWWtE4",
        "outputId": "33045042-ed3d-4787-a01e-987479740682"
      },
      "outputs": [],
      "source": [
        "# Notice how the Pruning Wrappers have been removed again!\n",
        "model_pruned_stripped.layers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2m-W_FhtWtE4"
      },
      "source": [
        "## Visualize Pruned Weights\n",
        "\n",
        "After the model has been pruned, it is helpful to visualize the pruned weights to examine the sparsity pattern achieved during the pruning process. This can provide insights into the impact of pruning on the model's internal structure.\n",
        "\n",
        "In this example, we demonstrate visualizing the pruned weights of the model. We also highlight the fact that setting `force_skip_layers=['dense_1/kernel:0']` in the pruning configuration led to the exclusion of the `dense_1/kernel:0` layer from pruning, resulting in no sparsity in that layer's weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFGTvDTlWtE4",
        "outputId": "18009748-5258-4af5-f94a-44f5243a24e2"
      },
      "outputs": [],
      "source": [
        "trainable_weights_dict = {weight.name: weight.numpy() for weight in model_pruned_stripped.trainable_weights}\n",
        "trainable_weights_dict.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lvFN30jaWtE5",
        "outputId": "3b05f018-7cc6-4e9b-9eba-b866f907d203"
      },
      "outputs": [],
      "source": [
        "\n",
        "base_path = 'prune_using_fit'\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "max_len_axis = 24\n",
        "for layer_name, layer in trainable_weights_dict.items():\n",
        "    title = f\"{layer_name}-shape-opxip-{layer.T.shape}\"\n",
        "    save_path = f\"{base_path}/{layer_name.replace('/', '-')}-prune-mask.png\"\n",
        "    plot_prune_mask(data=layer, axis_stride=pencil_size, max_xlen=max_len_axis, max_ylen=max_len_axis, title=title, save_path=save_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ad41ba704b490a86eec1bcebab6abb4e034d27c24fdf165a52d45ebef6dd8584"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
