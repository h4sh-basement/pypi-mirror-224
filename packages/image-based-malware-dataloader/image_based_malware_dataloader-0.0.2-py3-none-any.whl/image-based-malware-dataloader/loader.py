import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from torch import Tensor
from sklearn.model_selection import train_test_split
import torch
import pandas as pd
from os import listdir
import os
import random
from PIL import Image
from os.path import isfile, join

def load_mmcc_dataset_32_full():
    path_to_dataset = 'Datasets/MMCC/Images_32x32_split/train'
    path_to_labels = 'Datasets/MMCC/trainLabels.csv'
    df = pd.read_csv(path_to_labels)
    features = []
    full_labels = []
    for class_id in range(1, 10, 1):
        images = df.loc[(df["Class"] == class_id)]['Id'].values.tolist()
        images = [image + '.png' for image in images]
        image_files = []
        for f in images:
            try:
                image_files.append(Image.open(path_to_dataset + f'/{f}').convert('L'))
            except:
                print('Found non-existent file.')
        images_np = [np.asarray(img) for img in image_files]
        features.extend(images_np)
        full_labels.extend([class_id - 1 for i in range(len(images_np))])
    features = np.asarray([feature for feature in features])
    full_labels = np.array([label for label in full_labels])
    full_labels = full_labels.reshape((10829, 1))
    X_train, X_test, y_train, y_test = train_test_split(features, full_labels, test_size=0.1, shuffle=True)
    X_train = X_train.reshape((9746, 1, 32, 32))
    X_test = X_test.reshape((1083, 1, 32, 32))
    new_y_train = np.zeros((9746, 9))
    for ind, val in enumerate(y_train):
        tmp = np.zeros(9)
        tmp[val] = 1
        new_y_train[ind] = tmp
    y_train = new_y_train

    new_y_test = np.zeros((1083, 9))
    for ind, val in enumerate(y_test):
        tmp = np.zeros(9)
        tmp[val] = 1
        new_y_test[ind] = tmp
    y_test = new_y_test
    train_dataset = TensorDataset(Tensor(X_train), torch.argmax(Tensor(y_train), dim=1))
    trainloader = DataLoader(train_dataset, batch_size=256)

    test_dataset = TensorDataset(Tensor(X_test), torch.argmax(Tensor(y_test), dim=1))
    testloader = DataLoader(test_dataset, batch_size=256)
    return trainloader, testloader


def load_mmcc_dataset_32_train():
    path_to_dataset = 'Datasets/MMCC/Images_32x32_split/train'
    path_to_labels = 'Datasets/MMCC/trainLabels.csv'
    df = pd.read_csv(path_to_labels)
    features = []
    full_labels = []
    for class_id in range(1, 10, 1):
        images = df.loc[(df["Class"] == class_id)]['Id'].values.tolist()
        images = [image + '.png' for image in images]
        image_files = []
        for f in images:
            try:
                image_files.append(Image.open(path_to_dataset + f'/{f}').convert('L'))
            except:
                print('Found non-existent file.')
        images_np = [np.asarray(img) for img in image_files]
        features.extend(images_np)
        full_labels.extend([class_id - 1 for i in range(len(images_np))])
    features = np.asarray([feature for feature in features])
    full_labels = np.array([label for label in full_labels])
    full_labels = full_labels.reshape((9742, 1))
    X_train, X_test, y_train, y_test = train_test_split(features, full_labels, test_size=0.1, shuffle=True)
    X_train = X_train.reshape((8767, 1, 32, 32))
    X_test = X_test.reshape((975, 1, 32, 32))
    new_y_train = np.zeros((8767, 9))
    for ind, val in enumerate(y_train):
        tmp = np.zeros(9)
        tmp[val] = 1
        new_y_train[ind] = tmp
    y_train = new_y_train

    new_y_test = np.zeros((975, 9))
    for ind, val in enumerate(y_test):
        tmp = np.zeros(9)
        tmp[val] = 1
        new_y_test[ind] = tmp
    y_test = new_y_test
    train_dataset = TensorDataset(Tensor(X_train), torch.argmax(Tensor(y_train), dim=1))
    trainloader = DataLoader(train_dataset, batch_size=256)

    test_dataset = TensorDataset(Tensor(X_test), torch.argmax(Tensor(y_test), dim=1))
    testloader = DataLoader(test_dataset, batch_size=256)
    return trainloader, testloader


def train_benign_and_malimg_binary_dataset(
        path_to_dataset: str = 'Datasets/Benign-NET-MalImg',
        standardize: str = True,
        data_loader: bool = True):
    labels = ['Benign', 'Malicious']
    malware_labels = ['Adialer.C', 'Agent.FYI', 'Allaple.A', 'Allaple.L', 'Alueron.gen!J', 'Autorun.K', 'C2LOP.gen!g',
                      'C2LOP.P', 'Dialplatform.B', 'Dontovo.A', 'Fakerean', 'Instantaccess', 'Lolyda.AA1', 'Lolyda.AA2',
                      'Lolyda.AA3', 'Lolyda.AT', 'Malex.gen!J', 'Obfuscator.AD', 'Rbot!gen', 'Skintrim.N',
                      'Swizzor.gen!E',
                      'Swizzor.gen!I', 'VB.AT', 'Wintrim.BX', 'Yuner.A']
    features = []
    full_labels = []
    for index, label in enumerate(labels):
        if label == 'Benign':
            # path = path_to_dataset + '/Benign-NET-Images/train'
            path = path_to_dataset + '/Benign-NET-Images/train'
            types = [x[0] for x in os.walk(path)]
            files = [f for f in listdir(path) if isfile(join(path, f))]
            images = [Image.open(path + f'/{f}').convert('L') for f in files]
            images = [img.resize((32, 32)) for img in images]
            images_np = [np.asarray(img) for img in images]
            features.extend(images_np)
            [full_labels.append(0) for i in range(len(features))]
        else:
            for mal_label in malware_labels:
                path = path_to_dataset + f'/malimg_imgs_processed_32_train_test/train/{mal_label}'
                types = [x[0] for x in os.walk(path)]
                files = [f for f in listdir(path) if isfile(join(path, f))]
                images = [Image.open(path + f'/{f}').convert('L') for f in files]
                images_np = [np.asarray(img) for img in images]
                features.extend(images_np)
                [full_labels.append(1) for i in range(len(images_np))]
    features = np.asarray([feature for feature in features])
    full_labels = np.array([label for label in full_labels])
    idx = np.random.choice(len(full_labels), len(full_labels))
    full_labels = full_labels[idx]
    features = features[idx]
    X_train, X_test, y_train, y_test = train_test_split(features, full_labels, test_size=0.1)
    X_train = X_train.reshape((19215, 1, 32, 32))
    X_test = X_test.reshape((2135, 1, 32, 32))
    new_y_train = np.zeros((19215, 2))
    for ind, val in enumerate(y_train):
        tmp = np.zeros(2)
        tmp[val] = 1
        new_y_train[ind] = tmp
    y_train = new_y_train

    new_y_test = np.zeros((2135, 2))
    for ind, val in enumerate(y_test):
        tmp = np.zeros(2)
        tmp[val] = 1
        new_y_test[ind] = tmp
    y_test = new_y_test

    if data_loader:
        train_dataset = TensorDataset(Tensor(X_train), torch.argmax(Tensor(y_train), dim=1))
        trainloader = DataLoader(train_dataset, batch_size=128)

        test_dataset = TensorDataset(Tensor(X_test), torch.argmax(Tensor(y_test), dim=1))
        testloader = DataLoader(test_dataset, batch_size=128)
        return trainloader, testloader


def evaluate_bengin_and_malimg_binary_dataset(
        path_to_dataset: str = 'Datasets/Benign-NET-MalImg',
        standardize: str = True,
        data_loader: bool = True):
    print(os.getcwd())
    labels = ['Benign', 'Malicious']
    malware_labels = ['Adialer.C', 'Agent.FYI', 'Allaple.A', 'Allaple.L', 'Alueron.gen!J', 'Autorun.K', 'C2LOP.gen!g',
                      'C2LOP.P', 'Dialplatform.B', 'Dontovo.A', 'Fakerean', 'Instantaccess', 'Lolyda.AA1', 'Lolyda.AA2',
                      'Lolyda.AA3', 'Lolyda.AT', 'Malex.gen!J', 'Obfuscator.AD', 'Rbot!gen', 'Skintrim.N',
                      'Swizzor.gen!E',
                      'Swizzor.gen!I', 'VB.AT', 'Wintrim.BX', 'Yuner.A']
    features = []
    full_labels = []
    for index, label in enumerate(labels):
        if label == 'Benign':
            # path = path_to_dataset + '/Benign-NET-Images/train'
            path = path_to_dataset + '/Benign-NET-Images/train'
            types = [x[0] for x in os.walk(path)]
            files = [f for f in listdir(path) if isfile(join(path, f))]
            images = [Image.open(path + f'/{f}').convert('L') for f in files]
            images = [img.resize((32, 32)) for img in images]
            images_np = [np.asarray(img) for img in images]
            features.extend(images_np)
            [full_labels.append(0) for i in range(len(features))]
        else:
            for mal_label in malware_labels:
                path = path_to_dataset + f'/malimg_imgs_processed_32_train_test/train/{mal_label}'
                types = [x[0] for x in os.walk(path)]
                files = [f for f in listdir(path) if isfile(join(path, f))]
                images = [Image.open(path + f'/{f}').convert('L') for f in files]
                images_np = [np.asarray(img) for img in images]
                features.extend(images_np)
                [full_labels.append(1) for i in range(len(images_np))]
    features = np.asarray([feature for feature in features])
    full_labels = np.array([label for label in full_labels])
    idx = np.random.choice(len(full_labels), len(full_labels))
    full_labels = full_labels[idx]
    features = features[idx]
    X_train = features
    y_train = full_labels
    features = []
    full_labels = []

    for index, label in enumerate(labels):
        if label == 'Benign':
            # path = path_to_dataset + '/Benign-NET-Images/train'
            path = path_to_dataset + '/Benign-NET-Images/test'
            types = [x[0] for x in os.walk(path)]
            files = [f for f in listdir(path) if isfile(join(path, f))]
            images = [Image.open(path + f'/{f}').convert('L') for f in files]
            images = [img.resize((32, 32)) for img in images]
            images_np = [np.asarray(img) for img in images]
            features.extend(images_np)
            [full_labels.append(0) for i in range(len(features))]
        else:
            for mal_label in malware_labels:
                path = path_to_dataset + f'/malimg_imgs_processed_32_train_test/test/{mal_label}'
                types = [x[0] for x in os.walk(path)]
                files = [f for f in listdir(path) if isfile(join(path, f))]
                images = [Image.open(path + f'/{f}').convert('L') for f in files]
                images_np = [np.asarray(img) for img in images]
                features.extend(images_np)
                [full_labels.append(1) for i in range(len(images_np))]
    features = np.asarray([feature for feature in features])
    full_labels = np.array([label for label in full_labels])
    idx = np.random.choice(len(full_labels), len(full_labels))
    full_labels = full_labels[idx]
    features = features[idx]
    X_test = features
    y_test = full_labels
    X_train = X_train.reshape((21350, 1, 32, 32))
    X_test = X_test.reshape((2385, 1, 32, 32))
    new_y_train = np.zeros((21350, 2))
    for ind, val in enumerate(y_train):
        tmp = np.zeros(2)
        tmp[val] = 1
        new_y_train[ind] = tmp
    y_train = new_y_train

    new_y_test = np.zeros((2385, 2))
    for ind, val in enumerate(y_test):
        tmp = np.zeros(2)
        tmp[val] = 1
        new_y_test[ind] = tmp
    y_test = new_y_test
    train_dataset = TensorDataset(Tensor(X_train), torch.argmax(Tensor(y_train), dim=1))
    trainloader = DataLoader(train_dataset, batch_size=128)

    test_dataset = TensorDataset(Tensor(X_test), torch.argmax(Tensor(y_test), dim=1))
    testloader = DataLoader(test_dataset, batch_size=128)
    return trainloader, testloader


def get_unbalanced_benign_net(path_to_dataset: str = 'Datasets/Benign-NET-MalImg'):
    # Sample all the benign images dataset
    features = []
    full_labels = []
    path = path_to_dataset + '/Benign-NET-Images/test'
    types = [x[0] for x in os.walk(path)]
    files = [f for f in listdir(path) if isfile(join(path, f))]
    images = [Image.open(path + f'/{f}').convert('L') for f in files]
    images = [img.resize((32, 32)) for img in images]
    images_np = [np.asarray(img) for img in images]
    features.extend(images_np)
    [full_labels.append(0) for i in range(len(features))]

    benign_set_size = len(features)
    return features, full_labels, benign_set_size


def unbalanced_benign_net_mmcc_dataset(path_to_dataset_base: str = 'Datasets/Benign_NET_MMCC'):
    features, full_labels, benign_set_size = get_unbalanced_benign_net(path_to_dataset_base)

    # Ingest the MMCC dataset
    path_to_dataset = path_to_dataset_base + '/MMCC/Images_32x32_split/train'
    path_to_labels = path_to_dataset_base + '/MMCC/trainLabels.csv'
    df = pd.read_csv(path_to_labels)
    mmcc_features = []
    mmcc_full_labels = []
    for class_id in range(1, 10, 1):
        images = df.loc[(df["Class"] == class_id)]['Id'].values.tolist()
        images = [image + '.png' for image in images]
        image_files = []
        for f in images:
            try:
                image_files.append(Image.open(path_to_dataset + f'/{f}').convert('L'))
            except:
                print('Found non-existent file.')
        images_np = [np.asarray(img) for img in image_files]
        mmcc_features.extend(images_np)
        mmcc_full_labels.extend([class_id - 1 for i in range(len(images_np))])
    mmcc_features = np.asarray([feature for feature in mmcc_features])
    mmcc_full_labels = np.array([label for label in mmcc_full_labels])

    # Randomly select the number of malware samples
    malware_samples = random.randrange(1, 10)
    # Get the indices of the malware samples
    malware_sample_indices = random.sample(range(0, len(mmcc_features)), malware_samples)
    # Get the malware samples of the randomly selected indices
    malware_sample_features = [mmcc_features[i] for i in malware_sample_indices]
    # Get the malware labels of the randomly selected indices
    malware_sample_labels = [mmcc_full_labels[i] for i in malware_sample_indices]
    # Get the malware names of the randomly selected indices
    malware_sample_names = malware_sample_labels
    # Add the malware samples to the features
    features.extend(malware_sample_features)
    # Add the malware labels to the labels
    [full_labels.append(1) for i in range(malware_samples)]

    features = np.asarray([feature for feature in features])
    full_labels = np.array([label for label in full_labels])
    idx = np.random.choice(len(full_labels), len(full_labels))
    full_labels = full_labels[idx]
    features = features[idx]
    X_test = features
    y_test = full_labels

    X_test = X_test.reshape((X_test.shape[0], 1, 32, 32))

    new_y_test = np.zeros((X_test.shape[0], 2))
    for ind, val in enumerate(y_test):
        tmp = np.zeros(2)
        tmp[val] = 1
        new_y_test[ind] = tmp
    y_test = new_y_test

    test_dataset = TensorDataset(Tensor(X_test), torch.argmax(Tensor(y_test), dim=1))
    testloader = DataLoader(test_dataset, batch_size=128)

    return testloader, malware_sample_names, benign_set_size


def unbalanced_benign_net_malimg_dataset(path_to_dataset: str = 'Datasets/Benign-NET-MalImg'):
    malware_labels = ['Adialer.C', 'Agent.FYI', 'Allaple.A', 'Allaple.L', 'Alueron.gen!J', 'Autorun.K', 'C2LOP.gen!g',
                      'C2LOP.P', 'Dialplatform.B', 'Dontovo.A', 'Fakerean', 'Instantaccess', 'Lolyda.AA1', 'Lolyda.AA2',
                      'Lolyda.AA3', 'Lolyda.AT', 'Malex.gen!J', 'Obfuscator.AD', 'Rbot!gen', 'Skintrim.N',
                      'Swizzor.gen!E',
                      'Swizzor.gen!I', 'VB.AT', 'Wintrim.BX', 'Yuner.A']

    features, full_labels, benign_set_size = get_unbalanced_benign_net(path_to_dataset)

    # Sample all the malware images dataset
    malimg_features = []
    malimg_labels = []
    for index, mal_label in enumerate(malware_labels):
        path = path_to_dataset + f'/malimg_imgs_processed_32_train_test/test/{mal_label}'
        types = [x[0] for x in os.walk(path)]
        files = [f for f in listdir(path) if isfile(join(path, f))]
        images = [Image.open(path + f'/{f}').convert('L') for f in files]
        images_np = [np.asarray(img) for img in images]
        malimg_features.extend(images_np)
        [malimg_labels.append(index) for i in range(len(images_np))]

    # Randomly select the number of malware samples
    malware_samples = random.randrange(1, 10)
    # Get the indices of the malware samples
    malware_sample_indices = random.sample(range(0, len(malimg_features)), malware_samples)
    # Get the malware samples of the randomly selected indices
    malware_sample_features = [malimg_features[i] for i in malware_sample_indices]
    # Get the malware labels of the randomly selected indices
    malware_sample_labels = [malimg_labels[i] for i in malware_sample_indices]
    # Get the malware names of the randomly selected indices
    malware_sample_names = [malware_labels[i] for i in malware_sample_labels]
    # Add the malware samples to the features
    features.extend(malware_sample_features)
    # Add the malware labels to the labels
    [full_labels.append(1) for i in range(malware_samples)]

    features = np.asarray([feature for feature in features])
    full_labels = np.array([label for label in full_labels])
    idx = np.random.choice(len(full_labels), len(full_labels))
    full_labels = full_labels[idx]
    features = features[idx]
    X_test = features
    y_test = full_labels

    X_test = X_test.reshape((X_test.shape[0], 1, 32, 32))

    new_y_test = np.zeros((X_test.shape[0], 2))
    for ind, val in enumerate(y_test):
        tmp = np.zeros(2)
        tmp[val] = 1
        new_y_test[ind] = tmp
    y_test = new_y_test

    test_dataset = TensorDataset(Tensor(X_test), torch.argmax(Tensor(y_test), dim=1))
    testloader = DataLoader(test_dataset, batch_size=128)

    return testloader, malware_sample_names, benign_set_size


def evaluate_malimg_dataset(path_to_dataset: str = 'Datasets/MalImg/malimg_imgs_processed_32_train_test',
                            standardize: str = True,
                            data_loader: bool = True):
    labels = ['Adialer.C', 'Agent.FYI', 'Allaple.A', 'Allaple.L', 'Alueron.gen!J', 'Autorun.K', 'C2LOP.gen!g',
              'C2LOP.P', 'Dialplatform.B', 'Dontovo.A', 'Fakerean', 'Instantaccess', 'Lolyda.AA1', 'Lolyda.AA2',
              'Lolyda.AA3', 'Lolyda.AT', 'Malex.gen!J', 'Obfuscator.AD', 'Rbot!gen', 'Skintrim.N', 'Swizzor.gen!E',
              'Swizzor.gen!I', 'VB.AT', 'Wintrim.BX', 'Yuner.A']
    features = []
    full_labels = []
    for index, label in enumerate(labels):
        path = path_to_dataset + f'/train/{label}'
        files = [f for f in listdir(path) if isfile(join(path, f))]
        images = [Image.open(path + f'/{f}').convert('L') for f in files]
        images_np = [np.asarray(img) for img in images]
        features.extend(images_np)
        full_labels.extend([index for i in range(len(images_np))])
    features = np.asarray([feature for feature in features])
    full_labels = np.array([label for label in full_labels])
    full_labels = full_labels.reshape((8394, 1))
    idx = np.random.choice(8394, 8394)
    full_labels = full_labels[idx]
    features = features[idx]
    X_train = features
    y_train = full_labels
    features = []
    full_labels = []
    for index, label in enumerate(labels):
        path = path_to_dataset + f'/test/{label}'
        files = [f for f in listdir(path) if isfile(join(path, f))]
        images = [Image.open(path + f'/{f}').convert('L') for f in files]
        images_np = [np.asarray(img) for img in images]
        features.extend(images_np)
        full_labels.extend([index for i in range(len(images_np))])
    features = np.asarray([feature for feature in features])
    full_labels = np.array([label for label in full_labels])
    full_labels = full_labels.reshape((945, 1))
    idx = np.random.choice(945, 945)
    full_labels = full_labels[idx]
    features = features[idx]
    X_test = features
    y_test = full_labels
    X_train = X_train.reshape((8394, 1, 32, 32))
    X_test = X_test.reshape((945, 1, 32, 32))
    new_y_train = np.zeros((8394, 25))
    for ind, val in enumerate(y_train):
        tmp = np.zeros(25)
        tmp[val] = 1
        new_y_train[ind] = tmp
    y_train = new_y_train

    new_y_test = np.zeros((945, 25))
    for ind, val in enumerate(y_test):
        tmp = np.zeros(25)
        tmp[val] = 1
        new_y_test[ind] = tmp
    y_test = new_y_test
    train_dataset = TensorDataset(Tensor(X_train), torch.argmax(Tensor(y_train), dim=1))
    trainloader = DataLoader(train_dataset, batch_size=512)

    test_dataset = TensorDataset(Tensor(X_test), torch.argmax(Tensor(y_test), dim=1))
    testloader = DataLoader(test_dataset, batch_size=512)
    return trainloader, testloader


def load_malimg_dataset_img(path_to_dataset: str = 'Datasets/MalImg/malimg_imgs_processed_32',
                            standardize: str = True,
                            data_loader: bool = True, final_train: bool = False):
    labels = ['Adialer.C', 'Agent.FYI', 'Allaple.A', 'Allaple.L', 'Alueron.gen!J', 'Autorun.K', 'C2LOP.gen!g',
              'C2LOP.P', 'Dialplatform.B', 'Dontovo.A', 'Fakerean', 'Instantaccess', 'Lolyda.AA1', 'Lolyda.AA2',
              'Lolyda.AA3', 'Lolyda.AT', 'Malex.gen!J', 'Obfuscator.AD', 'Rbot!gen', 'Skintrim.N', 'Swizzor.gen!E',
              'Swizzor.gen!I', 'VB.AT', 'Wintrim.BX', 'Yuner.A']
    features = []
    full_labels = []
    for index, label in enumerate(labels):
        path = path_to_dataset + f'/{label}'
        files = [f for f in listdir(path) if isfile(join(path, f))]
        images = [Image.open(path + f'/{f}').convert('L') for f in files]
        images_np = [np.asarray(img) for img in images]
        features.extend(images_np)
        full_labels.extend([index for i in range(len(images_np))])
    features = np.asarray([feature for feature in features])
    full_labels = np.array([label for label in full_labels])
    if not final_train:
        full_labels = full_labels.reshape((8394, 1))
    else:
        full_labels = full_labels.reshape((9339, 1))
    X_train, X_test, y_train, y_test = train_test_split(features, full_labels, test_size=0.1)
    if final_train:
        X_train = X_train.reshape((8405, 1, 32, 32))
        X_test = X_test.reshape((934, 1, 32, 32))
        new_y_train = np.zeros((8405, 25))
        for ind, val in enumerate(y_train):
            tmp = np.zeros(25)
            tmp[val] = 1
            new_y_train[ind] = tmp
        y_train = new_y_train

        new_y_test = np.zeros((934, 25))
        for ind, val in enumerate(y_test):
            tmp = np.zeros(25)
            tmp[val] = 1
            new_y_test[ind] = tmp
        y_test = new_y_test
    else:
        X_train = X_train.reshape((7554, 1, 32, 32))
        X_test = X_test.reshape((840, 1, 32, 32))
        new_y_train = np.zeros((7554, 25))
        for ind, val in enumerate(y_train):
            tmp = np.zeros(25)
            tmp[val] = 1
            new_y_train[ind] = tmp
        y_train = new_y_train

        new_y_test = np.zeros((840, 25))
        for ind, val in enumerate(y_test):
            tmp = np.zeros(25)
            tmp[val] = 1
            new_y_test[ind] = tmp
        y_test = new_y_test

    if data_loader:
        train_dataset = TensorDataset(Tensor(X_train), torch.argmax(Tensor(y_train), dim=1))
        trainloader = DataLoader(train_dataset, batch_size=128)

        test_dataset = TensorDataset(Tensor(X_test), torch.argmax(Tensor(y_test), dim=1))
        testloader = DataLoader(test_dataset, batch_size=128)
        return trainloader, testloader


# Method has been adapted from
# https://github.com/UmerTariq1/Malware-classification-paper-implementation-using-Convolutional-Neural-Network
def load_microsoft_malware_classification_challenge_dataset_training_dataset_split \
                (valid_ids: list = [], path_to_dataset: str = 'Datasets/MMCC', save_im: bool = True):
    """
        :param valid_ids: The list of dictionaries that holds ids and classes that need to be formed into images
        :param path_to_dataset: String Value
        :return: X_train, X_test, y_train, y_test
    """

    counter = 1
    data = []
    for element in valid_ids:
        FileToOpen = path_to_dataset + '/train/'
        FileToOpen += element['ID']
        FileToOpen += '.bytes'
        with open(FileToOpen) as file:
            out = file.readlines()
            count = 0
            arr = ''
            for line in out:
                line2 = line.split(' ', 1)[1]
                line3 = line2.split(' ')
                for l in line3:
                    try:
                        binval = '{:08b}'.format(int(l, 16))
                        arr += str(binval)
                    except:
                        count += 1

            arr2 = [arr[start:start + 8] for start in range(0, len(arr), 8)]

            # making decimal values and saving them in file
            decval_list = []
            for a in arr2:
                decval = int(a, 2)
                decval_list.append(decval)
                # file2.write(str(decval)+" ")

            lengthoflist = len(decval_list)

            matrix_size = np.floor(np.sqrt(lengthoflist))

            width = int(matrix_size)
            # height = int(lengthoflist/16)
            height = int(matrix_size)
            Matrix = np.zeros(shape=(height, width))

            wcount = 0
            hcount = 0
            count = 0
            for li in decval_list:
                try:
                    Matrix[hcount][wcount] = li
                except:
                    print("File used : ", FileToOpen, ". Number of values used ", count, ", out of ", lengthoflist)
                    break
                wcount += 1
                count += 1
                if wcount == width:
                    hcount += 1
                    wcount = 0
            decMat = np.array(Matrix)
            if save_im:
                save_image(decMat, element['ID'])
            else:
                data.append(decMat)
        print(counter, " files have been processed")
        counter += 1
        print()
    data = np.array(data)
    print(data.shape)
    print("Preprocessing is  done")


def get_all_mmcc_data(path_to_dataset: str = 'Datasets/MMCC'):
    """
        :param path_to_dataset: String Value
        :return: A list of all data in the train folders and their labels
    """
    # Read the training dataset
    df = pd.read_csv(path_to_dataset + '/trainLabels.csv')
    # Gets all files in the training directory
    files = [f for f in listdir(path_to_dataset + '/train')]
    # files = [f for f in listdir(path_to_dataset + '/train') if isfile(join(path_to_dataset + '/train', f))]
    # Removes all .asm files from the list
    files = [i for i in files if '.asm' not in i]
    # obtain all rows from df that are in files
    valid_ids = []
    for row in df.iterrows():
        if row[1]['Id'] + '.bytes' in files:
            valid_ids.append({
                'ID': row[1]['Id'],
                'Class': row[1]['Class']
            })
    return valid_ids


def ids_to_tensor(valid_ids: list = None, test_split: int = 0.1):
    """
        :param test_split: The split to show how much data should be in the testing and training sets
        :param valid_ids: The list of dictionaries that holds ids and classes that need to be formed into images
        :return: Tensor containing data from the valid ids
    """
    test_split = int(len(valid_ids) * (1 - test_split))
    train_set = valid_ids[:test_split]
    test_set = valid_ids[test_split:]
    X_train = []
    X_test = []
    y_train = []
    y_test = []
    for img in train_set:
        X_train.append(img['ID'])
        y_train.append(img['Class'])
    for img in test_set:
        X_test.append(img['ID'])
        y_test.append(img['Class'])

    return torch.Tensor(X_train), torch.Tensor(X_test), torch.Tensor(np.array(y_train)), torch.Tensor(np.array(y_test))


def save_image(element, path_to_dataset: str = 'Datasets/MMCC'):
    FileToOpen = path_to_dataset + '/train/'
    FileToOpen += element['ID']
    FileToOpen += '.bytes'
    with open(FileToOpen) as file:
        out = file.readlines()
        count = 0
        arr = ''
        for line in out:
            line2 = line.split(' ', 1)[1]
            line3 = line2.split(' ')
            for l in line3:
                try:
                    binval = '{:08b}'.format(int(l, 16))
                    arr += str(binval)
                except:
                    count += 1

        arr2 = [arr[start:start + 8] for start in range(0, len(arr), 8)]

        # making decimal values and saving them in file
        decval_list = []
        for a in arr2:
            decval = int(a, 2)
            decval_list.append(decval)
            # file2.write(str(decval)+" ")

        lengthoflist = len(decval_list)

        matrix_size = np.floor(np.sqrt(lengthoflist))

        width = int(matrix_size)
        # height = int(lengthoflist/16)
        height = int(matrix_size)
        Matrix = np.zeros(shape=(height, width))

        wcount = 0
        hcount = 0
        count = 0
        for li in decval_list:
            try:
                Matrix[hcount][wcount] = li
            except:
                print("File used : ", FileToOpen, ". Number of values used ", count, ", out of ", lengthoflist)
                break
            wcount += 1
            count += 1
            if wcount == width:
                hcount += 1
                wcount = 0
        decMat = np.array(Matrix)
    im = Image.fromarray(decMat)
    if im.mode != 'RGB':
        im = im.convert('RGB')
    im = im.save(path_to_dataset + '/Images/train/' + element["ID"] + '.png')
    print('Saved Image: ' + path_to_dataset + '/Images/train/' + element["ID"] + '.png')

